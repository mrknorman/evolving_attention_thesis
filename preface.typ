#set page(numbering: "i", number-align: center)
#counter(page).update(1)
#set heading(numbering: "I.a")

// Summary
= Summary

This thesis investigates the application of machine learning to gravitational-wave data analysis. Primarily, it focuses on artificial neural networks, but it also presents work to optimize the design and application of these networks with genetic algorithms, another machine learning method. This method of hyperparameter optimisation was utilised to design models for a low-latency burst search pipeline, MLy. Along with the use of genetic algorithms for hyperparameter optimisation, work is also performed to test the performance of attention-based networks on two gravitational wave data analysis tasks, compact binary coalescence detection, and estimating parameters of overlapping pairs of compact binary coalescences.

@gravitational-waves-sec introduces gravitational wave science, in order to contextualize the data analysis problems examined throughout. 

@machine-learning-sec examines the underlying principles of artificial neural networks and demonstrates a simple toy example to demonstrate the effectiveness of neural networks as a data analysis method. 

@application-sec then explores the intersection between the two introduced fields. It presents the methodology used for training dataset generation throughout the thesis, introducing the custom software developed in order to enable rapid dataset generation and iteration. It also contains a review of previous work that has been done to use artificial neural networks for gravitation wave data analysis, as well as a series of experiments to demonstrate the ineffectiveness of unspecialized artificial neural networks for the task. This section concludes with recreations of some important results from the literature which act as a comparative baseline for the rest of the thesis.

@dragonn-sec presents Dragonn, a genetic algorithm that can act as a general optimisation method for neural networks in gravitational wave data science, capable of optimising the network, the training dataset, and the training procedure simultaneously, and allowing for the easy addition of new hyperparameters. @skywarp-sec presents experiments to test the effectiveness of attention-based networks for gravitational-wave analysis, more specifically, compact binary coalescence detection. It demonstrates a marginal improvement over the recreated convolutional neural networks from the literature presented in @application-sec. 

Finally, @crosswave-sec expands the exploration of attention-based models to investigate cross-attention between multiple gravitational-wave detector outputs. We use this novel approach to examine the problem of parameter estimation on overlapping signals. 

#pagebreak()

// Table of contents.
#show outline: set heading(numbering: "I.a")
#outline(depth: 3, indent: true)
#pagebreak()

// Figures
= List of Figures
  - *@newtons_law:* An illustration of Newton's law of universal gravitation, as described by @newtons-law-of-universal-gravitation. Two particles, here distinguished by their unique masses, $m_1$ and $m_2$, are separated by a distance, $r$. According to Newton's law, they are pulled toward each other by the force of gravity acting on each object $F$ @principia_mathematica, each being pulled directly toward the other by a force that is equal and opposite to its partner's.
  - *@absolute_time_and_space:* An illustration of two competing historical views on the nature of space and time @newton_vs_leibniz. _Left:_ Newton's vision of absolute universal time and absolute space, wherein time moves forward at a constant and uniform rate across the universe and space is immobile and uniform. In this model, both time and space can exist independently of objects within, even in an entirely empty universe. _Right:_ Leibniz's view proposed a wherein time and space did not and could not exist independently of the objects used to measure them. Within this model, space is simply a measure of the relative distances between objects, and time is a measure of the relative motion of objects as their relative positions change. In this model, it makes little sense to talk of a universe without objects since time and space do not exist without objects with relative positions and velocities.
  - *@light_clock_diagram:* An illustration of the light clock thought experiment. The light clock thought experiment is a scenario that can be imagined in order to illustrate the apparent contradiction that arises from a universally constant speed of light. In order to rectify this contradiction, the concepts of time dilation, and length contraction are introduced, fundamentally changing our understanding of the nature of time and space. Two observers stand in inertial reference frames, from special relativity we know all inertial reference frames are equal, and the laws of physics, including the speed of light, should look identical @light_clock @special_relativity.
  - *@flat:* Two depictions of Einsteins's spacetime. For illustrative purposes, since we are not 4D beings and the paper on which this will be printed very much isn't, the four dimensions of our universe have been compacted down into two. It should also be noted that these illustrations were not generated with correct physical mathematics but only to give an impression of the concepts being described. _Left:_ Minkowski space --- in the absence of any mass, spacetime will not experience any curvature @gravitation. This is the special case that Einstien's special relativity describes. If we were to place a particle into this environment, it would not experience any acceleration due to gravity. If the particle were massive, it would distort the spacetime, and the spacetime would no longer be considered Minkowski space even though, alone, the particle would not experience any acceleration. Often, when dealing with particles of low mass, their effects on the distortion of spacetime are ignored, and we can still describe the scenario with special relativity @special_relativity. _Right:_ Spacetime distorted by a massive object, shown in blue. Curved space is described by Einstein's more general theory, general relativity @gravitation. In this scenario, we can see how the presence of mass imprints a distortion into the shape of spacetime. Any particles also present in the same universe as the blue object, assuming it has existed indefinitely, will experience an apparent acceleration in the direction of the blue sphere. A beam of light, for example, comprised of photons and entirely massless, would be deflected when moving past the sphere. Even though light will always travel in a straight line through the vacuum of space, the space itself is distorted; therefore, a straight line path will manifest itself as an apparent attraction toward the sphere. Notice that using Newton's universal law of gravitation, @newtons-law-of-universal-gravitation, the mass of the photon is zero; therefore, it should not experience any gravitational attraction, and indeed, gravitational lensing of the passage of starlight, as it moves past the sun, was one of the first confirmations of Einstein's theory of general relativity @gravitational_lensing. Were this sphere several thousand kilometres in diameter, any lifeforms living on its surface, which would appear essentially flat at small scales, would experience a pervasive and everpresent downward force. Note that the mass of the object is distributed throughout its volume, so in regions near the centre of the sphere, the spacetime can appear quite flat, as equal amounts of mass surround it from all directions. 
  - *@gravitaional-potentials:* Two further depictions of spacetime. Again, these images are a 2D representation of 4D spacetime, and they were generated without correct physical descriptions but for illustrative purposes alone. _Left:_ Two objects, one in blue with a lesser mass and one in yellow with a greater mass. Objects with a larger mass distort spacetime to a greater extent, so objects close to the yellow sphere will experience a stronger apparent acceleration as the space curves and the objects continue to move on a straight line. In this scenario, if stationary, the yellow and blue objects will accelerate and move toward each other and, without outside interference, inevitably collide. However, if either the blue or yellow ball is given an initial velocity perpendicular to the direction of the other sphere so that its straight-line path orbits the other sphere, they can remain equidistant from each other in a stable orbit for potentially very long periods of time. As we will see, this orbit will eventually lose energy and decay, but depending on the masses of the two objects, this could take an extremely long time. _Right:_ A black hole. Thus far, we have assumed that the mass of the yellow and blue objects are evenly distributed through their volume, so the spacetime at the very centre of the object is, at its limit, entirely flat. In many scenarios, this is a physically possible arrangement of matter, as although gravity pulls on every particle within the object, pulling it toward the centre, it is a very weak pull compared to the other forces of nature, which push back out and stop the particles continuing on their naturally preferred trajectory. This prevents a complete collapse of the object. Gravity, however, has one advantage on its side, and that is that there is no negative mass, only positive, so whereas large bodies tend to be electrically neutral as positive and negative charges cancel each other out, gravity always grows stronger. If enough mass congregates in the same place, or if the forces pushing matter away from the centre stop, there's nothing to stop gravity from pulling every particle in that object right to the centre, right into a singular point of mass with infinite density known as the singularity. As this collapse occurs, the curvature of spacetime surrounding the object gets stronger and stronger, eventually reaching the point where within a region around the singularity, known as the event horizon, all straight-line paths point toward the singularity. Meaning that no matter your speed, no matter your acceleration, you cannot escape, even if you are light itself. Consequently, no information can ever leave the event horizon, and anything within is forever censored from the rest of the universe.
  - *@orbits-diagram:* Two illustrations of scenarios involving simple orbital mechanics. _Left:_ In this thought experiment we imagine a cannon atop a large mountain on an unphysically small spherical planet with mass, $m$. As is described in both Newtonian mechanics and general relativity, objects are attracted toward the center of mass of the planet. Left to their own devices they will fall until they meet some force resisting their motion, most likely, the surface of the planet. The cannon operator can control the velocity of the projected cannon balls. They note that the more velocity they impart, the longer it takes for the ball to impact the surface of the planet. The balls can travel further before impacting the ground when their velocity is greater, even if the time to impact remains the same. However, with this increased distance traveled along the surface of the sphere, the distance between the ball and the ground increases as the surface of the planet curves away from the ball. Eventually, the ball's trajectory will circularise around the planet, and, if not impeded by any other forces, the ball would remain on this circular trajectory indefinitely. _Right:_ Two identical massive objects, such as planets, in a circular orbit with a shared centre, called a barycentre (note that the objects do not have to have equal mass or be in a circular orbit, to have a shared barycentre, in fact, this will always be the case). Any massive objects can orbit each other, including black holes.
  - *@waves:* A depiction of the region of spacetime surrounding two inspiraling black holes. The spacetime grid visible is a 2D representation of the true 4D nature of our universe as described by general relativity @gravitation. This depiction was not produced by an accurate simulation but was constructed as a visual aid alone. Two massive objects can orbit each other if they have sufficient perpendicular velocity; this is a natural state for objects to find themselves trapped in because the chances of direct collisions between objects are low, and any objects that find themselves gravitationally bound together and do not experience a direct collision will eventuate in an orbit. The same is true for black holes; whether they form from pairs of massive stars that both evolve into black holes after the end of their main sequence lives or whether they form separately and through dynamical interaction, end up adjoined and inseparable, the occurrence of two black holes orbiting is not inconceivable @black_hole_binary_formation. Over time, small amounts of energy will leak from these binaries; ripples are sent out through the cosmos, carrying energy away from the system and gradually reducing the separation between the companions. As they get closer, the curvature of the spacetime they occupy increases, and thus, their acceleration toward each other grows. They speed up, and the amount of energy that is lost through gravitational radiation increases, further increasing the speed of their inspiral in an ever-accelerating dance. If they started just close enough, this process would be enough to merge them within the lifetime of the universe; they will inevitably collide with an incredible release of energy out through spacetime as powerful gravitational waves. It is these waves, these disturbances in the nature of length and time itself, that we can measure here on Earth using gravitational wave observatories.
  - *@wobble:* The effect of two polarisation states of gravitational waves as they oscillate whilst passing through a region of spacetime. Each of the black dots represents freely falling particles unrestricted by any other forces. The plus and cross polarisations shown are arbitrary names, and the polarisation can be at any angle, but plus and cross are a convention to distinguish the two orthogonal states.
  - *@interferometer_diagram*: A very simplified interferometer diagram. Real gravitational wave detection apparatus have considerably more optics than what is shown. The power recycling and signal recycling mirrors help maintain a high laser power within the cavities. Higher laser powers are preferable as they help reduce quantum shot noise, the limiting source of noise at high frequencies.
  - *@global_network:* Location of currently operation LIGO detectors: LIGO Livingston (L1), LIGO Hanford (H1), Virgo (V1), Kagra (K1), and GEO600 (G1) @open_data. Arm angles are accurate, the arm lengths were generated with a relative scale with the real detectors: 4 km for the two LIGO detectors, 3 km for Virgo and Kagra, and 600 m for GEO600. 
  - *@noise_diagram:* Full noise budget of the LIGO Hanford Observatory (LHO) during the 3#super("rd") joint observing run. This image was sourced from @ligo_o3_noise.
  - *@ai_relationships*: The loose hierarchical relationship between different umbrella terms used in artificial intelligence @deep_learning_review.
  - *@artificial_neuron_diagram:* _Upper_: The Artificial Neuron. This figure illustrates the operations that compose the archetypical artificial neuron, where $accent(x, arrow)$ is the input vector, $f$ is the activation function, $accent(w, arrow)$ is the weights vector, and b is the neuron bias. An artificial neuron takes an input vector, $accent(x, arrow)$, and performs some useful calculations (hopefully). Both the weights vector, $accent(w, arrow)$, and bias value, $b$, comprise the neuron's adjustable parameters, $accent(theta, arrow)$, that must be tuned for the neuron to perform any useful operations @artifical_neurons. _Note_: During computation, the bias, $b$, is not normally added in a separate operation; instead, it is added as an extra $x_0$ term included in the same calculation as the summation of the product of the weights, $accent(w, arrow)$, and input values, $accent(x, arrow)$. _Lower_: An abstraction of the more complicated interior structure of the artificial neuron. Abstraction is common and necessary when describing artificial neural networks as networks are often comprised of thousands if not millions of artificial neurons.
  - *@mnist_examples:* Example MNIST data @mnist. A single example of each of the ten classes within the MNIST example dataset. As can be seen, the classes range from zero to nine inclusive. Each example consists of a grid of 28 by 28 pixels containing one float value between 0.0 and 1.0. In the above image, values near one are represented as nearly white, and values near 0.0 as black. When ingested by our single-layer perception, they will be flattened into a 1D vector; see @flatten-sec.
  - *@single_layer_perceptron:* Various representations of a Single-Layer Perceptron or Single-Layer Artificial Neural Network. _Upper:_ Diagram illustrating the structure and operation of a single-layer perceptron. In the example shown, a handwritten zero is fed into the single-layer perceptron. The 2D image is first flattened into a 1D vector, see @flatten-sec; then, the entire vector is fed into each neuron. If the training process has worked correctly, each neuron will have learned to identify one of the possible classes, in this case, digits. As can be seen from the output values, $accent(accent(y, hat), arrow) = [accent(y, hat)_0, ... arrow accent(y, hat)_9]$, which are taken from a real trained model, this model can correctly identify this input as a zero with high confidence. _Middle:_ An abridged version of the upper diagram demonstrating the operation of feeding a handwritten one into the perceptron. This shows how future network diagrams will be abstracted for simplicity and that the perceptron outputs a different, correct value when it ingests a one rather than a zero. _Lower:_ A further abstraction of the network. This type of abstraction will be used commonly throughout this thesis when dealing with networks consisting of multiple layers. A dense layer, wherein all neurons are attached to all previous neurons, will be shown as a filled black rectangle, and the icon next to it represents that the activation function applied is a softmax activation function @softmax_ref; see @softmax-sec.
  - *@gradient_example:* An illustration of gradient descent, where $accent(nabla, arrow) L_(M accent(x, arrow) accent(y, arrow)) (accent(theta, arrow))$ is the loss at a fixed model architecture, $M$, input vector $accent(x, arrow)$, and data label $accent(y, arrow)$. This simplified example of the shape of a 1D parameter space shows how the gradient of the loss function with respect to the model parameters can be used to move toward the minimum of the loss function. The shape of the loss function in this example is given by $ L_(M accent(x, arrow) accent(y, arrow)) (accent(theta, arrow)) = theta^2$. In almost all cases, the parameter space will be much more complex than the one depicted in both dimensionality and shape complexity. Usually, the shape of the loss function will be an N-dimensional surface, where N is the number of parameters, $accent(theta, arrow)$, in the model, but the principle is still the same. For a 2D example of a gradient space; see @gradient_descent_examples. This plot can be recreated with the code found here: https://github.com/mrknorman/data_ad_infinitum/blob/main/chapter_x_gradient_descent.ipynb.
  - *@perceptron_history:* _Upper:_ The performance of the single layer perceptron model described in @training_arificial_neurons over 15 epochs, where one epoch consists of training the model on all training examples in the MNIST dataset of handwritten Arabic numerals @mnist. The model loss is defined as the categorical cross-entropy of the model's output vector, $accent(accent(y, hat), arrow)$ and the ground-truth label, $accent(y, arrow)$, whereas the accuracy metric is defined as the number of examples in the test dataset that are correctly classified, where a correct classification is any output with 50 per cent or more probability in the correct class. _Lower_: Two examples of less successful classifications. The left example would still be measured as a successful classification by our accuracy metric, whereas the right example would be marked as an unsuccessful classification. 
  - *@perceptron_parameters:* Learned model parameters. Each artificial neuron in our single-layer perception is represented by a labelled parameter map shaped into the same dimensions as the input images. These maps show the learned weight values that correspond to each pixel of the input images. Very little structure can be made out by the human eye. Perhaps in the weight maps for the zero-classifier neuron, we can see an area toward the centre of the map that is negatively weighted. This might be expected as there are rarely high-value pixels at the centre of the circular zero. A similar but opposite effect might also be attributed to the one-classifier, where the centre of the image often contains high-value pixels. In general, unless you squint very hard, it is difficult to make out patterns in the parameters. This "black-box" effect means that after even one more layer is added to the network, it becomes very difficult to determine the action of dense layer neurons intuitively.
  - *@multi-layer-perceptron:* _Upper:_ Diagram of a multi-layer network with one output layer and one hidden layer. The non-linear computation introduced by the ReLU activation function applied to the hidden layer allows this network to solve considerably more complex problems than the previously described single-layer perceptron model. _See @relu-sec _.  As can be seen, by the displayed output, which again is taken from a real instance of a trained model, this network has no problem classifying the previously difficult image of a five. _Lower:_ An abstraction of the same model.
  - *@multi-layer-perceptron_history:* The performance of the multi-layer perceptron model described in @together_strong over 15 epochs. As can be seen in comparison to @perceptron_history, the training is both faster and with a better final result.
  - *@rnn:* _ Left: _ The generalised dense feed-forward artificial neural network. Where $T$ is the number of hidden layers in your network, $H$ is the number of neurons at that layer, $N$, is the number of elements in the input vector, $accent(x, arrow)$, and $O$ is the number of elements in the output vector $accent(accent(y, hat), arrow)$. As can be seen in the diagram, the number of hidden layers in your network is unconstrained, as is the number of neurons in each of those layers, which should be noted does not have to be the same. This is opposed to the output layer, which must have the same number of neurons as is expected by your loss function. _ Right _ A very simple illustration of a recurrent neural network. This network illustrates the retroactive data flow that is possible in a recurrent neural network. In this example, the output of the network from one inference operation is added to the input of the next inference operation. It should be noted that this is a very naive implementation of a recurrent neural network. In actuality, the networks usually have a much more complex structure, such as LSTMs (Long Short Term Memory) networks.
  - *@activation_functions:* Four of the most common activation functions. _Upper Left:_ A linear activation function. In this case, the slope, k, is 1, meaning that the shape of the output is unchanged vs the input. _Upper Right:_ Sigmoid activation function, a special case of the logistic activation function, which limits the output value between 0 and 1. _Lower Left:_ ReLU (Rectified Linear Unit) activation function and its variants, an easy way to provide non-linearity to multi-layer networks. _Lower Right:_ SoftMax activation function. In the case of multi-neuron outputs, when using softmax, the output of each neuron depends on the value of each other neuron. For this reason, the simplest non-trivial case, where the length of the output vector, ùëÅ, is 2, has been chosen, and the outputs are represented on a 3D plot. This figure can be recreated with the notebook found at: https://colab.research.google.com/github/mrknorman/data_ad_infinitum/blob/main/chapter_x_activation_functions.ipynb.
  - *@gradient_descent_examples:* _ Left: _ An idealised gradient descent path. This gradient descent process quickly reaches the true function minimum where, in this case, the loss is close to zero. However, this is a constructed example by first finding a point near the function minimum and performing a gradient ascent operation. _ Right: _ A more realistic gradient descent path. This example shows a simple but real gradient descent function running on the cost function. As can be seen, it takes many more steps and has not yet converged on the true minimum; in fact, the process might be at risk of getting stuck in a local minimum. Both examples were generated with using this notebook: https://github.com/mrknorman/data_ad_infinitum/blob/main/chapter_x_gradient_descent.ipynb.
  - *@flattening_diagram:* A flattening layer. This layer takes a 2D input matrix $X = mat(x^1_1, x^1_2; x^2_1, x^2_1)$ and converts it into a 1D vector, $ accent(y, arrow) = [y_1, y_2, y_3, y_4]$, without using any learned parameters or altering the values of the data. It simply rearranges the indexes and removes all but one dimension. Reshaping layers are a more general version of a flattening layer, where an input vector or matrix can be transformed into any equivalently sized output vector or matrix.
  - *@data_features:* A non-exhaustive hierarchical depiction of some of the features, and proposed features, of gravitational-wave interferometer data. The first fork splits the features into two branches, representing the duration of the features. Here, *continuous* features are defined as features for which it is extremely unlikely for us to witness their start or end within the lifespan of the current gravitational-wave interferometer network and probably the current scientific community @continious_gravitational_waves. These features have durations anywhere from thousands to billions of years. *Transient* features have comparatively short durations @first_detction, from fractions of seconds in the case of stellar-mass Binary Black Hole (BBH) mergers @first_detction to years in the case of supermassive BBH mergers @supermassive_mergers. It should be noted that the detectable period of supermassive binaries could be much longer; although the mergers themselves are transient events, there is no hard cut-off between the long inspiral and the merger event. Nevertheless, the mergers are probably frequent enough that some will end within the lifetime of the proposed LISA space constellation, so in some cases, they can be considered transients @supermassive_mergers. The next fork splits features by origin. Features of *astrophysical* origin originate from beyond Earth. This distinction is practically synonymous with the distinction between gravitational waves and signals from other sources since no other astrophysical phenomena are known to have a similar effect in interferometers @first_detction. Features of *terrestrial* origin, unsurprisingly, originate from Earth. These primarily consist of detector glitches caused by seismic activity or experimental artifacts @det_char. Astrophysical transients have a further practical division into CBCs and bursts. The category of *bursts* contains all astrophysical transients that are not CBCs @bursts_O1. The primary reason for this distinction is that CBCs have been detected and have confirmed waveform morphologies @first_detction @first_bns. As of the writing of this thesis, no gravitational-wave burst events have been detected @bursts_O1 @bursts_O2 @bursts_O3. Bursts often require different detection techniques @cWB @x-pipeline; of the proposed sources, many are theorised to have waveforms with a much larger number of free parameters than CBCs, as well as being harder to simulate as the physics are less well-understood @supernovae_waveforms_2 @starquake_detection. These two facts compound to make generating large template banks for such signals extremely difficult. This means that coherence detection techniques that look for coherent patterns across multiple detectors are often used over matched filtering @x-pipeline @oLIB @cWB @BayesWave @MLy. The astrophysical leaves of the diagram represent possible and detected gravitational-wave sources; the text's colourings represent their current status. Green items have been detected using gravitational-wave interferometers, namely the merger of pairs of Binary Black Holes (BBHs) @first_detction, Binary Neutron Stars (BNSs) @first_bns, or one of each (BHNSs) @first_nsbh; see @GWTC-1 @GWTC-2 @GWTC-3 for full catalogues of detections. Yellow items have been detected via gravitational waves but using Pulsar Timing Arrays (PTAs) rather than interferometers @PTA. Blue items represent objects and systems that are theorised to generate gravitational waves and have been detected by electromagnetic observatories but not yet with any form of gravitational wave detection. This includes white dwarf binaries @white_dwarf_binary_em @white_dwarf_lisa_detection, the cosmological background @cosmological_background_em @cosmological_background_gw, starquakes @starquake_em @starquake_gw, and core-collapse supernovae CCSN @supernovae_em @supernovae_gw. This is because they are too weak and/or too uncommon for our current gravitational-wave detector network to have had a chance to detect them. Finally, red items are possible, theorised sources of gravitational waves that have not yet been detected by any means. These are, evidently, the most contentious items presented, and it is very possible that none of these items will ever be detected or exist at all. It should be noted that the number of proposed sources in this final category is extensive, and this is far from an exhaustive list. The presented proposed continuous sources are neutron star asymmetries @neutron_star_gw_review, and the presented transient sources are extraterrestrial intelligence @et_gw, cosmic string kinks and cusps @cosmic_string_cusps, accretion disk instabilities @accrection_disk_instability, domain walls @domain_walls, and nonlinear memory effects @non_linear_memory_effects.
  - *@psd-example:* Examples of Power Spectral Density (PSD) transforms. _Left:_ Two time domain series. The red series is a #box("20" + h(1.5pt) + "Hz") wave with a duration of #box("0.7" + h(1.5pt) + "s"), and the blue series is this same time series concatenated with a #box("40" + h(1.5pt) + "Hz") wave from $t = 0.7#h(1.5pt)s$ onwards. _Right:_ The two PSDs of the time series are displayed in the left panel. The red PSD was performed across only the #box("0.7" + h(1.5pt) + "s") of the red wave's duration, whereas the blue PSD was taken over the full #box("2.0" + h(1.5pt) + "s") duration. As can be seen, the blue PSD has two peaks, representing the two frequencies of the two waves combined to make the blue time series --- each peak is lower than the red peak, as they are averaged across the full duration, and their respective heights are proportional to their durations as both waves have the same amplitude and vary only in duration.
  - *@noise_comparison:* One-second examples of the four possible types of simulated and real noise considered by this thesis. Where real noise is used, it is taken from the LIGO Livingston detector during the third observing run at the GPS times listed. In order, from top to bottom, these are examples of white Gaussian noise, coloured Gaussian noise, pseudo-real noise, and real noise. A description of these noise types and their generation can be found in @noise_acquisition_sec. The left column shows the unaltered values of the noise. Note that the noise has been scaled in all cases except for the pure white noise, which is generated at the correct scale initially. This scaling is used to reduce precision errors and integrate more effectively with the machine learning pipeline, as most loss and activation functions are designed around signal values near unity; see @loss_functions_sec and @activation_functions_sec. The right column shows the same noise realisations after they have been run through a whitening filter. In each case, the PSD of a #box("16.0" + h(1.5pt) + "s") off-source noise segment not displayed is used to generate a Finite Impulse Response (FIR) filter, which is then convolved with the on-source data; see @feature-eng-sec. For the simulated and pseudo-real noise cases, the off-source data is generated using the same method as the on-source data but with a longer duration. In the real noise case, the off-source data consists of real interferometer data drawn from #box("16.5" + h(1.5pt) + "s") before the start of the on-source segment to #box("0.5" + h(1.5pt) + "s") before the start of the on-source segment. This 0.5s gap is introduced because #box("0.5" + h(1.5pt) + "s") must be cropped from the data following the whitening procedure in order to remove edge effects induced via windowing, as well as acting as a buffer to reduce contamination of the off-source data with any features present in the on-source data. Note that the whitened noise plots look very similar for the three simulated noise cases --- a close examination of the data reveals that there is some small variation between the exact values. This similarity occurs because the off-source and on-source noise segments for these examples are generated with identical random seeds and thus have identical underlying noise realisations (which can be seen exactly in the unwhitened white noise plot). Since the PSDs of the on-source and off-source data are nearly identical for the simulated cases, the whitening procedure is almost perfect and reverts it nearly perfectly to its white state. If anything, this similarity boosts confidence that our custom whitening procedure is operating as expected.
  - *@example_injections:* Eight simulated waveforms that could be used for injection into noise to form an obfuscated training, testing, or validation example for an artificial neural network. Note that only the plus polarisation component of the strain, $h_plus$, has been plotted in order to increase visual clarity. The leftmost four injections are IMRPhenomD waveforms generated using cuPhenom @cuphenom_ref; see @cuphenom-sec, with parameters (shown in the adjacent grey information boxes) drawn from uniform distributions between #box("5.0" + h(1.5pt) + $M_dot.circle$) and #box("95.0" + h(1.5pt) + $M_dot.circle$) for the mass of both companions and between -0.5 and 0.5 for the dimensionless spin component. Note that during injection generation, the two companions are always reordered so that the mass of companion one is greater and that the IMRPhenomD waveform ignores the x and y spin components. They are included just for code completion. The rightmost four injections consist of WNB waveforms generated via the method described in @injection-gen-sec. Their parameters are again drawn from uniform distributions and are shown in the grey box to their right. The durations are limited between #box("0.1"+ h(1.5pt) + "s") and #box("1.0" + h(1.5pt) + "s"), and the frequencies are limited to between #box("20.0" + h(1.5pt) + "Hz") and #box("500.0" + h(1.5pt) + "Hz"), with the minimum and maximum frequencies automatically swapped.
  - *@supernovae_example:* The plus polarisation component of the gravitational-wave strain of a simulated core-collapse supernova at a distance of #box("10" + h(1.5pt) + "kpc"), this data was taken from @supernovae_waveforms. Although some structures can clearly be observed, it is possible to imagine that a method trained to detect WNB signals, such as those presented in @example_injections, might be able to detect the presence of such a signal.
  - *@projection_examples:* Example projection of two artificial gravitational-wave waveforms. The blue waveforms have been projected into the LIGO Livingston interferometer, the red waveforms have been projected into the Ligo Hanford interferometer, and the green waveforms have been projected into the VIRGO interferometer. The left column displays different projections of an IMRPhenomD waveform generated with the cuPhenom GPU library @cuphenom_ref; see @cuphenom-sec. The right column displays different projections of a WNB waveform generated with the method described in @injection-gen-sec. The projections are performed using a GPU adaptation of the PyCBC Python library's @pycbc project_wave function. Both waveforms are projected from different source locations; the projection and time displacement are different in each case.
  - *@scaling_comparison:* Eight examples of artificial injections scaled to a particular scaling metric and added to a real noise background to show variance between different scaling methods. The blue line demonstrates the whitened background noise plus injection; the red line represents the injection after being run through the same whitening transform as the noise plus injection, and the green line represents the injection after scaling to the desired metric. The leftmost column contains an IMRPhenomD waveform, generated using @cuphenom_ref, injected into a selection of various background noise segments and scaled using SNR; see @snr-sec. From upper to lower, the SNR values are 4, 8, 12, and 16, respectively. The rightmost column displays a WNB injected into various noise distributions, this time scaled using $h_op("rss")$; see @hrss-sec. From upper to lower, the $h_op("rss")$ values are as follows: $8.52 times 10^(-22)$, $1.70 times 10^(-21)$, $2.55 times 10^(-21)$, and $3.41 times 10^(-21)$. As can be seen, though both sequences are increasing in linear steps with a uniform spacing of their respective metrics, they do not keep in step with each other, meaning that if we double the optimal SNR of a signal, the $h_op("rss")$ does not necessarily also double.
  - *@layout_options:* Possible data layouts for multi-detector examples. Here, $d$ is the number of included detectors, and $N$ is the number of input elements per time series. There are three possible ways to align interferometer time-series data from multiple detectors. These layouts are discussed in more detail in @dim_sec.
  - *@whitnening_examples:* An example of a segment of interferometer data before and after whitening. The two leftmost plots in blue show the PSD, _upper_, and raw data, _lower_, output from the LIGO Hanford detector before any whitening procedure was performed. The two rightmost plots show the same data after the whitening procedure described in @whitening-sec has been implemented. The data was whitened using the ASD of a #box("16.0" + h(1.5pt) + "s") off-source window from #box("16.5" + h(1.5pt) + "s") before the start of the on-source window to #box("0.5" + h(1.5pt) +"s") before. The #box("0.5" + h(1.5pt) +"s") gap is introduced as some data must be cropped after whitening due to edge effects caused by windowing. This also acts to ensure that it is less likely that any features in the on-source data contaminate the off-source data, which helps reduce the chance that we inadvertently whiten any interesting features out of the data.
  - *@onsource_offsource_regions:* Demostration of the on-source and off-source regions used to calculate the ASD used during the whitening operations throughout this thesis wherever real noise is utilised. Where artificial noise is used, the off-source and on-source segments are generated independently but with durations equivalent to what is displayed above. The blue region shows the #box("16.0" + h(1.5pt) + "s") off-source period, the green region shows the #box("1.0" + h(1.5pt) + "s") on-source period, and the two red regions represent the #box("0.5" + h(1.5pt) + "s") crop periods, which are removed after whitening. During an online search, the on-source region would advance in second-long steps, or if some overlap was implemented, less than second-long steps, meaning all data would eventually be searched. The leading #box("0.5" + h(1.5pt) + "s") crop region will introduce an extra #box("0.5" + h(1.5pt) + "s") of latency to any search pipeline. It may be possible to avoid this latency with alternate whitening methods, but that has not been discussed here. 
  - *@pearson_example:* Example whitened on-source and correlation plots of real interferometer noise from a pair of detectors, in this case, LIGO Livingston and LIGO Hanford, with either coherent, incoherent, or no injections added. The leftmost plots adjacent to the info panels are grouped into pairs. In each case, LIGO Livingston is at the top, and LIGO Hanford is underneath. Identical on-source and off-source noise segments are used for each example of the same detector, and noise for each detector was gathered with a time difference of no more than #box("2048.0" + h(1.5pt) + "s"). In the leftmost plots, the green series is the unwhitened but projected waveform to be injected into the real noise from that detector. The red series is that same injection but subject to the same whitening procedure that will also be applied to the on-source plus injections, and the blue series is the whitened on-source plus injections. The rightmost plots each correspond to a pair of detectors and display the rolling Pearson correlation values between those two whitened on-source plus injection series. Since there is approximately a max arrival time difference of #box("0.01" + h(1.5pt) + "s") between LIGO Livingston and LIGO Hanford, the number of correlation calculations performed corresponds to the rounded number of samples required to represent #box("0.02" + h(1.5pt) + "s") of data at #box("2048.0" + h(1.5pt) + "Hz"). This number is two times the maximum arrival time difference because the difference could be positive or negative. In this case, that difference comes to 40 samples. All injections have been scaled to an optimal network SNR of 30 using the method described in @snr-sec. The upper pair of detectors has no injection. As would be expected, the correlation is low regardless of the assumed arrival time difference. The second pair from the top has been injected with a coherent white noise burst (WNB), see @injection-gen-sec, which has been projected onto the two detectors using a physically realistic mechanism previously described in @projection-sec. Here, the correlation is much stronger. We can see it rise and fall as the waveforms come in and out of coherence. The third from the top, the central plot, shows an injection of two incoherent WNBs. They are processed identically to the coherent case, but the initial waveforms are generated independently, including their durations. The Pearson correlation looks very similar to the pure noise case in the uppermost plot, as might be expected. The second from the lowest pair has been injected with a coherent IMRPhenomD waveform, which again has been correctly projected. We can observe that a small correlation is observed at an arrival time difference of around #box("0.005" + h(1.5pt) + "s"), suggesting that the two waveforms arrived at the detectors #box("0.005" + h(1.5pt) + "s") apart. Finally, the lowest plot depicts two incoherent IMRPhenomD waveforms projected into the noise. Though these are generated with different parameters, the shared similarities in morphology between all CBC waveforms cause correlation to be registered. By maximum amplitude alone, it may even appear as though there is more correlation happening here than in the correlated case. This highlights one potential weakness of using the Pearson correlation, which can sometimes show some degree of correlation even if the two waveforms are not produced using the same physically simulated mechanism.
  - *@spectrogram_examples:* Six example noise segments and their corresponding spectrograms. In all cases, the noise is real interferometer data acquired from the LIGO Hanford detector during the 3#super("rd") observing run. It is whitened using the procedure described in @whitening-sec. For the time series plots, the green series represents the original, unwhitened waveform before injection, the red series is the waveform with the same whitening transform applied to it as was applied to the on-source background plus injection, and the blue series is the whitened on-source background plus injection, except for the first two time series plots which contain no injection. The spectrograms are generated using the STFT described by @stft-eq, converted into power with @stft_sq, and finally transformed into a decibel logarithmic scale for plotting using @dec-eq. The two uppermost plots and their respective spectrograms have no injections. The two middle plots and their respective spectrograms have IMRPhenomD @imrphenom_d approximants created with cuPhenom injected into the noise @cuphenom_ref, and the two lower plots and their respective spectrograms, have White Noise Burst (WNB) waveforms generated using the method described in @injection-gen-sec, injected into the noise. In all cases, the injections are scaled to an optimal SNR randomly selected between 15 and 30; these are quite high values chosen to emphasize the features in the spectrograms. As can be seen, the whitened noise that contains injected features has spectrograms with highlighted frequency bins that have a magnitude much larger than the surrounding background noise; the different signal morphologies also create very different shapes in the spectrograms. This allows us to see the frequency components of the signal more easily, observe the presence of interesting features, and differentiate between the WNB and the CBC case.
  - *@perceptron-cbc-architectures:* Perceptron diagrams. The four different architectures used to test the use of purely dense models for both the single-detector CBC detection case and the multi-detector burst detection problem. The only differences are that the input vector sizes are different between the cases: `(NUM_EXAMPLES_PER_BATCH, NUM_SAMPLES)` in the case of the single detector CBC search and `(NUM_EXAMPLES_PER_BATCH, NUM_DETECTORS, NUM_SAMPLES)` in the multi-detector coherent burst search. All models take in two input vectors into a custom-designed GWFlow whitening layer, the off-source and the on-source vectors; see @whitening-sec for more information about the whitening procedure, and all models are capped with a dense layer with a single output neuron that is used to feed the binary loss function, with a sigmoid activation function. Each hidden layer has been tested with 64, 128, and 256 neurons, and one hidden layer was tested with 512 as a sample with higher neuron counts:  _Top:_ Zero-hidden layer model. _Second to top:_ Two-hidden layer model. _Second to bottom:_ Three-hidden layer model. _Bottom:_ One hidden layer model.
  - *@perceptron_single_accuracy:* The accuracy history of perceptron models training to detect IMRPhenomD waveforms generated using cuPhenom @cuphenom_ref that have been obfuscated by real interferometer noise sampled from the LIGO Livingston detector during the 3#super("rd") observing run. Visit #link("http://tinyurl.com/ypu3d97m") for interactive plots, whilst they're still working. The optimal SNR of waveforms injected into the training and validation sets was uniformly distributed between 8 and 15. Input was from a single detector only. A rough search was performed over a relatively arbitrary selection of model architectures, which varied the number of layers and the number of perceptrons in each layer. The architectures of each model can be seen in the figure legends as a list of numbers where each digit is the number of artificial neurons in that layer. All are trained with the same training hyperparameters, details of which can be found in @perceptron-training-parameters. Each epoch consisted of $10^5$ training examples, and it should be noted that, unlike the regular training pipelines, each training epoch consisted of newly generated waveforms injected into unseen noise segments, though the validation examples are consistent. Training of each model was halted after ten consecutive epochs with no improvement to validation loss, the values of which are shown in @perceptron_single_loss. Validation noise was drawn from a separate pool of data segments inaccessible to the training data loader. We can see that the maximum accuracy achieved by any perceptron model only approaches 75%. Although these validations are performed with a pool containing mixed waveform SNRs and at an unrestrained False Alarm Rate (FAR) (this accuracy uses a score threshold of 0.5 regardless of FAR), it is clear that this is insufficient to be useful. _Upper:_ Plot of model accuracies when measured with training data ($10^5$ epoch-unique examples). _Lower:_ Plot of model accuracies when mesured with validation data ($10^4$ epoch-consistent examples).
  - *@perceptron_single_loss:* Training loss history of perceptron models training to detect IMRPhenomD waveforms generated using cuPhenom @cuphenom_ref, obfuscated by real interferometer noise from the LIGO Livingston detector from the 3#super("rd") observing run. The loss is computed using binary cross entropy loss function and is used by the gradient descent algorithm, in this case, the Adam optimizer, as a minimization target. It also acts as the monitor by which the pipeline knows to stop the training process early. If the pipeline detects that the validation model loss has not decreased in more than 10 epochs, training is halted. Visit #link("http://tinyurl.com/ypu3d97m") for interactive plots. See @perceptron_single_accuracy for a more detailed description of the training data. _Upper:_ Plot of model loss when measured with training data ($10^5$ epoch-unique examples). _Lower:_ Plot of model loss when mesured with validation data ($10^4$ epoch-consistent examples).
  - *@perceptron_single_far:* Perceptron False Alarm Rate (FAR) curves. This plot was created by running each of our 14 models over a pure noise validation dataset of $10^5$ noise examples. A relatively small number of noise examples are used due to the observed inaccuracy of the models during training which suggested that they would not be able to reach low FAR scores and thus would not necessitate a larger validation dataset. The output scores of the model from each inference over the pure noise validation dataset are sorted and plotted on this graph. The x-axis is the output score of the model inference on that example of noise. The y-axis is calculated by using @FAR_index_calc and provides the estimated number of false alarms that the model would output per second of pure noise data given the threshold score displayed on the x-axis. We can use this graph to calculate positive result thresholds for our classifier, at different false alarm rates. Once again, the models are listed with the number of artificial neurons in each hidden layer. Visit #link("http://tinyurl.com/2wkaarkh") to view an interactive plot. 
  - *@perceptron_efficiency_curves_single:* Perceptron efficiency curves. For each of the 14 perceptron models trained, 31 efficiency tests are performed at evenly spaced optimal SNR values between 0 and 15. For each test, 8192 examples with signals of the relevant SNR are examined by the model, and the percentage of those that scored above the threshold was plotted, see @specificity, for three different False Alarm Rate (FAR) thresholds: #box("0.1"+ h(1.5pt) + "Hz"), #box("0.01"+ h(1.5pt) + "Hz"), and #box("0.001"+ h(1.5pt) + "Hz"). The efficiency curve for each FAR threshold is presented on a unique plot. Some models have been excluded, they are shaded grey on the legends, because they are incapable of performing any classification at the chosen FAR thresholds. Visit #link("http://tinyurl.com/2wkaarkh") to view an interactive plot. . _Upper:_ Efficiency curves at a FAR of #box("0.1" + h(1.5pt) + "Hz"). _Middle:_ Efficiency curves at a FAR of #box("0.01" + h(1.5pt) + "Hz"). _Lower:_ Efficiency curves at a FAR of #box("0.001" + h(1.5pt) + "Hz").
  - *@perceptron_roc_curve:* Reciever Operator Curve (ROC) Curve at $rho_"opt" = 8$. To create this plot a validation dataset containing waveforms all of an SNR of eight was generated. The ability of the model to detect these waveforms was then measured at different FARs. All models show very similar, poor performance. Visit #link("http://tinyurl.com/2wkaarkh") to view an interactive plot.
  - *@perceptron_multi_accuracy:* The accuracy history of perceptron models training to detect multi-detector WNBs generated using GWFlow and obfuscated by real interferometer noise sampled from the LIGO Livingston and LIGO Hanford detectors during the 3#super("rd") observing run. Visit ADD_LINK for interactive plots. The optimal SNR of waveforms injected into the training and validation sets was uniformly distributed between 12 and 30. The input was generated using real noise from LIGO Hanford and LIGO Livingston. The training procedure was identical to the single detector case, except for the SNR range increase and the multiple detector data supply. We can see in these training plots, that despite the increased SNR range, training and validation accuracy barely creep above 50% (which can be achieved by random selection). This indicates that dense networks are even less suited for the more complex coherence detection problem. Further validation will be performed for completion. Visit #link("http://tinyurl.com/4jj3t5fj") to view an interactive plot. _Upper:_ Plot of model accuracies when measured with training data ($10^5$ epoch-unique examples). _Lower:_ Plot of model accuracies when tested with validation data ($10^4$ epoch-consistent examples).
  - *@perceptron_multi_loss:* The loss history of perceptron models training to detect multi-detector WNBs generated using GWFlow and obfuscated by real interferometer noise sampled from the LIGO Livingston and LIGO Hanford detectors during the 3#super("rd") observing run. Visit ADD_LINK for interactive plots. The optimal SNR of waveforms injected into the training and validation sets was uniformly distributed between 12 and 30. The input was generated using real noise from LIGO Hanford and LIGO Livingston. The losses show a similar picture to the accuracy plots, and although we see a gradual decline it is very shallow and triggers the patience early stopping before it has had any chance to gain significant performance, assuming that is even possible. Patience could be increased, but as we will see in later architectures, this is not competitive. _Upper:_ Plot of model losses when measured with training data ($10^5$ epoch-unique examples). _Lower:_ Plot of model losses when tested with validation data ($10^4$ epoch-consistent examples). Visit #link("http://tinyurl.com/4jj3t5fj") to view an interactive plot.
  - *@perceptron_multi_far:* Perceptron False Alarm Rate (FAR) curves. This plot was created by running each of our 14 models over a pure noise validation dataset of $10^5$ noise examples. Performance is low across the board demonstrating that dense layer perceptrons are unsuitable for this kind of WNB detection, at least within the hyperparameter range tested. Visit #link("http://tinyurl.com/bdz9axpf") to view an interactive plot.
  - *@perceptron_efficiency_curves_multi:* Perceptron efficiency curves for the multi-detector WNB detection model. For each of the 14 perceptron models trained, 31 efficiency tests are performed at evenly spaced optimal SNR values between 0 and 30. For each test, 8192 examples with signals of the relevant SNR are examined by the model. The percentage of those that scored above the threshold is plotted, see @specificity, for two different False Alarm Rate (FAR) thresholds: #box("0.1"+ h(1.5pt) + "Hz") and #box("0.01"+ h(1.5pt) + "Hz"), lower FARs are excluded due to small accuracies. _Upper:_ Efficiency curves at a FAR of #box("0.1" + h(1.5pt) + "Hz"). _Lower:_ Efficiency curves at a FAR of #box("0.01" + h(1.5pt) + "Hz"). Visit #link("http://tinyurl.com/bdz9axpf") to view an interactive plot.
  - *@perceptron_roc_curve_multi:* Reciever Operator Curve (ROC) Curve at an optimal SNR of eight. To create this plot a validation dataset containing waveforms all of an SNR of eight was generated. The ability of the model to detect these waveforms was then measured at different FARs. Again, all models show very similar, poor performance. Visit #link("http://tinyurl.com/bdz9axpf") to view an interactive plot.
  - *@kernel_example:* Diagram of a single kernel, $""_1^1k$, in a single convolutional layer. In this example, a 1D vector is being input; therefore, the single kernel's output is also 1D. This kernel has a kernel size of three, meaning that each neuron receives three input values from the layer's input vector, $accent(x, arrow)$, which in this case is length five. This means there is room for three repeats of the kernel. Its parameters are identical for each iteration of $""_1^1k$ at a different position. This means that if a pattern of inputs recognised by the kernel at position 1, $""_1^1k_1$ is translated two elements down the input vector, it will be recognised similarly by the kernel at $""_1^1k_3$. Although this translational invariance is only strict if the translation is a whole pixel multiple and no subsampling (pooling, stride, or dilation) is used in the network, this pseudo-translational invariance can be useful, as often, in images and time series data, similar features can appear at different spatial or temporal locations within the data. For example, in a speech classification model, a word said at the start of the time series can be recognised just as easily by the same pattern of parameters if that word is said at the end of the time series (supposing it lies on the sample pixel multiple). Thus, the same kernel parameters and the same filter can be repeated across the time series, reducing the number of parameters needed to train the model. This particular kernel would have $3 + 1 = 4$ total parameters, as it applied to a 1D input vector, and has a kernel size of three, with an additional parameter for the neuron bias. With only a single kernel, only one feature can be learned, which would not be useful in all but the most simple cases. Thus, multiple kernels are often used, each of which can learn its own filter.
  - *@kernel-size:* Illustration of how different values of kernel size would be laid out on a $4 times 4$ input image. In each case, unused input image values are shown as empty black squares on the grid, and input values read by the kernel are filled in red. The grids show the input combinations that a single kernel would ingest if it has a given size, assuming a stride value of one and zero dilation. The kernel sizes are as follows: _Upper left:_ $2 times 2$. _Upper right:_ $3 times 2$. _Lower left:_  $2 times 3$. _Lower right:_  $3 times 3$. One pixel in the output map is produced for each kernel position. As can be seen, the size of the output map produced by the kernel depends both on the input size and the kernel size; smaller kernels produce a larger output vector.
  - *@multi_kernel_example:* _Upper:_ Diagram of three convolutional kernels, $[""_1^1k, ""_2^1k, ""_3^1k]$, in a single convolutional layer. Each kernel is coloured differently, in red, green, and blue. Artificial neurons of the same colour will share the same learned parameters. Again, a 1D vector is being input; therefore, the output of each of the kernels is 1D, and the output of the kernels stack to form a 2D output vector, with one spatial dimension retained from the input vector and an extra discrete depth dimension representing the different features learned by each of the kernels. Again, each kernel has a kernel size of three. Multiple kernels allow the layer to learn multiple features, each of which can be translated across the input vector, as with the single kernel. Using @conv-layer-size, this layer would have $3 times ((1 times 3) + 1) = 12$ trainable parameters. It should be noted that this is a very small example simplified for visual clarity; real convolutional networks can have inputs many hundreds or thousands of elements long and thus will have many more iterations of each kernel, as well as many more kernels sometimes of a much larger size. _Lower:_ Abstracted diagram of the same layer with included hyperparameter information. 
  - *@multi_cnn_layer_example:* _Upper:_ Diagram of two convolutional layers, each with independent kernels. The first layer has three kernels, each with a size of three. The second layer has two kernels, both with a size of two. Again, this is a much-simplified example that would probably not have much practical use. Different kernels are coloured differently, in red, green, and blue. Although it should be noted that similar colours across layers should not be taken as any relationship between kernels in different layers, they are each tuned independently and subject to the whims of the gradient descent process. This example shows how the kernels in the second layer take inputs across the entire depth of the first layer but behave similarly along the original dimension of the input vector. In theory, the deeper layer can learn to recognise composite features made from combinations of features previously recognised by the layers below and visible in the output feature maps of the different kernels. This multi-layer network slice would have $(3 times ((1 times 3) + 1)) + (2 times ((3 times 2) + 1)) = 26$ total trainable parameters. This was calculated by applying @conv-layer-size to each layer. _Lower:_ Abstracted diagram of the same layers with included hyperparameter information. 
  - *@cnn_diagram:* _Upper:_ Diagram of a very simple convolutional neural network binary classifier consisting of four layers with tunable parameters plus one infrastructure layer without parameters. Two consecutive convolutional layers ingest the five-element input vector, $accent(x, arrow)$. The 2D output of the latter of the two layers is flattened into a 1D vector by a flattening layer. This flattened vector is then ingested by two dense layers, the latter of which outputs the final classification score. The first convolutional layer has three convolutional kernels, each with a size of three, and the second convolutional layer has two kernels, both with a size of two. The first dense layer has three artificial neurons, and the final output dense layer has a number of neurons dictated by the required size of the output vector. In the case of binary classification, this is either one or two. Different kernels within a layer are differentiated by colour, in this case, red, green, or blue, but a similar colour between layers does not indicate any relationship. Dimensionless neurons are shown in black; it should be noted that after flattening, dimensional information is no longer necessarily maintained by the network structure. Of course, no information is necessarily lost either, as the neuron index itself contains information about where it originated, so, during training, this information can still be used by the dense layers; it is just not necessarily maintained as it is in convolutional layers. This network will have in total $26 + (3 times 4 + 4) + (2 times 3 + 2) = 50$ trainable parameters. This network is very simple and would probably not have much practical use in real-world problems other than straightforward tasks that would probably not necessitate using neural networks. _Lower:_ Abstracted diagram of the same model with included hyperparameter information. 
  - *@stride:* Illustration of how different values of kernel stride would be laid out on a $4 times 4$ input image. In each case, unused input image values are shown as empty black squares on the grid, and input values read by the kernel are filled in red. Similar to kernel size, different values of stride result in a different output vector size. The strides shown are as follows: _Upper left:_ $1, 1$. _Upper right:_ $2, 1$. _Lower left:_  $1, 2$. _Lower right:_  $2, 2$.
  - *@dilation:* Diagram illustrating how different values of kernel dilation affect the arrangement of the kernel input pixels. In this example, the receptive field of a single $3 times 3$ kernel at three different dilation levels is displayed; differing colours represent the input elements at each dilation level. The shaded red kernel illustrates dilation level zero; the shaded blue region is a kernel with dilation of one, and the green kernel has a kernel dilation of two.
  - *@padding:* Diagram illustrating how padding can be added to the edge of an input vector in order to allow for otherwise impossible combinations of kernel, stride, size, and dilation. In each case, unused input image values are shown as empty black squares on the grid, input values read by the kernel are shaded red, and empty blue squares are unused values added to the original input vector, containing either zeros or repeats of the closest data values. In this example, the kernel size is $3 times 3$, and the kernel stride is $2, 2$.
  - *@george_diagram:* The two CNN architectures presented in George _et al._ @george_huerta_cnn. _Upper:_ Smaller model. _Lower:_ Larger model.
  - *@gabbard_diagram:* CNN architecture from Gabbard _et al._ @gabbard_messenger_cnn.
  - *@cnn_single_accuracy:* The accuracy history of attempts to retrain Convolutional Neural Networks (CNNs) with architectures adapted from the literature using the GWFlow pipeline. A custom GWFlow whitening layer has been added to the start of each model in order to reproduce the whitening data conditioning step applied in the original studies. The structure of the models is otherwise identical. Differences in the training and validation procedures, however, may lead to slightly different results than in the original studies. Rather than exactly attempting to mimic the datasets and training process used in each of these studies, it has been kept consistent with the other results throughout the thesis, in order to facilitate comparison. The models presented are the two models from George _et al._ @george_huerta_cnn, labeled "George Small", and "George Large", to differentiate them in terms of parameter count, and the single model from Gabbard _et al._ @gabbard_messenger_cnn. The network structure of these models can be seen in @george_diagram and @gabbard_diagram, respectively. The training and validation datasets were maintained from the perceptron single-detector training experiment. The dataset contains IMRPhenomD waveforms generated using cuPhenom @cuphenom_ref injected into real interferometer noise sampled from the LIGO Livingston detector during the 3#super("rd") joint observing run @O2_O3_DQ. The optimal SNR of waveforms injected into the training and validation sets was uniformly distributed between 8 and 15. Input was from a single detector only. Each epoch consisted of $10^5$ training examples, and it should be noted that, unlike regular training pipelines, each training epoch consisted of newly generated waveforms injected into unseen noise segments, though the validation examples are consistent. Training of each model was halted after ten consecutive epochs with no improvement to validation loss, the values of which are shown in @cnn_single_loss. Validation noise was drawn from a separate pool of data segments inaccessible to the training data loader. It is immediately clear that this is a huge improvement over the perceptron models, and it makes it evident why we abandon the idea of perceptrons so quickly. Both the training and validation accuracies jump to above 90% almost immediately, and in the case of the model from Gabbard _et al._, and the largest of the models from George _et al._, they plateau at approximately 98% accuracy, with only marginal improvements from there. The smaller model from George _et al._ plateaus closer to 96% accuracy. Considering approximants from both the training and validation datasets are generated with CBCs drawn uniformly between an optimal SNR of 8 and 15, this demonstrates good performance. Because two of the models plateau at statistically similar accuracies with quite different architectures, it suggests that they are approaching the detectability limit in both cases. An interesting examination will be to compare their performance with FAR-calibrated detection thresholds. _Upper:_ Plot of model accuracies when measured with training data ($10^5$ epoch-unique examples). Visit #link("http://tinyurl.com/mwxfvp33") for interactive plots. _Lower:_ Plot of model accuracies when measured with validation data ($10^4$ epoch-consistent examples).
  - *@cnn_single_loss:*  The loss history of attempts to retrain Convolutional Neural Networks (CNNs) from the literature using the GWFlow pipeline. These loss values correspond to the accuracies displayed in @cnn_single_accuracy. The models presented are the two models from George _et al._ @george_huerta_cnn, labeled "George Small", and "George Large", to differentiate them in terms of parameter count, and the single model from Gabbard _et al._ @gabbard_messenger_cnn. The network structure of these models can be seen in @george_diagram and @gabbard_diagram, respectively. The training and validation datasets were maintained from the perceptron single-detector training experiment. The dataset contains IMRPhenomD waveforms generated using cuPhenom @cuphenom_ref and real interferometer noise sampled from the LIGO Livingston detector during the 3#super("rd") joint observing run @O2_O3_DQ. The optimal SNR of waveforms injected into the training and validation sets was uniformly distributed between 8 and 15. Input was from a single detector only. Each epoch consisted of $10^5$ training examples, and it should be noted that, unlike regular training pipelines, each training epoch consisted of newly generated waveforms injected into unseen noise segments, though the validation examples are consistent. The loss is the metric used to determine when training is halted; this is done after ten epochs have passed with no improvement. Again we can see that this is a vast improvement over the perceptron case, see @perceptron_single_loss, at least in the time frame that is monitored, with loss values quickly falling to a region with a much smaller reduction gradient and then gradually improving from there with diminishing returns. It is these diminishing returns that can have a great impact on the ability of the model to sustain high accuracies with low FAR thresholds. Visit #link("http://tinyurl.com/mwxfvp33") for interactive plots. _Upper:_ Plot of model losses when measured with training data ($10^5$ epoch-unique examples). _Lower:_ Plot of model accuracies when measured with validation data ($10^4$ epoch-consistent examples).
  - *@cnn_far_single:* False Alarm Rate (FAR) plotted against the score threshold required to achieve that FAR, for three recreations of models from the literature. Two models are adapted from George _et al._ @george_huerta_cnn, labeled "George Small", and "George Large", to differentiate them in terms of model parameter count, and the single model from Gabbard _et al._ was also adapted. The network structure of these models can be seen in @george_diagram and @gabbard_diagram, respectively. The presented FAR curves are significantly lower than those achieved by the perceptrons in the single detector case, see @perceptron_single_far. This means that we will be able to achieve lower FARs with lower score thresholds, which typically, though not necessarily, leads to higher efficiencies at those FARs. We explore the efficiency results in @cnn_efficiency_curves_single. Visit #link("http://tinyurl.com/2s3dtd8a") for interactive plots.
  - *@cnn_efficiency_curves_single:* Model efficiency curves for three models adapted from the literature. Two models are adapted from George _et al._ @george_huerta_cnn, labeled "George Small", and "George Large", to differentiate them in terms of model parameter count, and the single model from Gabbard _et al._ @gabbard_messenger_cnn was also adapted. The network structure of these models can be seen in @george_diagram and @gabbard_diagram, respectively. These models verify that CNNs can achieve much higher accuracies within the training regime utilized, even when using threshold scores that are calibrated to specific False Alarm Rates (FARs). The perceptron efficiency curves for the single detector CBC detection case can be seen in @perceptron_efficiency_curves_single. They achieve higher accuracies almost across the board at the highest FARs depicted, #box("0.1" + h(1.5pt) + "Hz") and #box("0.01" + h(1.5pt) + "Hz"), except at SNRs where detection becomes virtually impossible ($<2$) in which case they perform similarly. They are also able to achieve results at lower FARs #box("0.001" + h(1.5pt) + "Hz") and #box("0.0001" + h(1.5pt) + "Hz"); at these FARs the perceptron models had negligible performance and were not depicted, so this is a significant improvement. Visit #link("http://tinyurl.com/2s3dtd8a") for interactive plots. _First:_ Efficiency curves at a FAR of #box("0.1" + h(1.5pt) + "Hz"). _Second:_ Efficiency curves at a FAR of #box("0.01" + h(1.5pt) + "Hz"). _Third:_ Efficiency curves at a FAR of #box("0.001" + h(1.5pt) + "Hz"). _Fourth:_ Efficiency curves at a FAR of #box("0.0001" + h(1.5pt) + "Hz").
  - *@roc_curves_single:* Reciever Operator Characteristic (ROC) curves, for three models adapted from the literature. Two models are adapted from George _et al._ @george_huerta_cnn, labeled "George Small", and "George Large", to differentiate them in terms of model parameter count, and the single model from Gabbard _et al._ @gabbard_messenger_cnn was also adapted. The network structure of these models can be seen in @george_diagram and @gabbard_diagram, respectively. In comparison with the ROC curves achieved by the perception models, see @perceptron_roc_curve, which at an optimal SNR of 8 looks to be almost randomly guessing, this is a significant improvement. The curves shown illustrate the model operating on a pool of injected signals at an optimal SNR of 8. Visit #link("http://tinyurl.com/2s3dtd8a") for interactive plots.
  - *@hyperparameter_space:* An example arbitrary hyperparameter space generated from a random mixture of Gaussians. The space presented here is 2D. In actuality, the space is likely to have a much larger dimensionality. Unlike in gradient descent where we are trying to minimize our loss, here we are trying to maximize our objective function, whatever we have determined that to be.
  - *@grid_search:* An example of the samples a grid search might use to find an optimal hyperparameter solution.
  - *@random_search:* An example of the samples a random search might use to find an optimal hyperparameter solution.
  - *@bayesian_descent_hp_optimisation:*  An example of the samples a Bayesian optimization might use to find an optimal hyperparameter solution. The descent method shown here has used a Gaussian Process to attempt to find the objective function maximum but has not done so particularly successfully. The method was not tuned to try and increase performance, as it was just for illustratory purposes.
  - *@mly_coincidence:* MLy Coincidence Model developed with Dragonn @MLy.
  - *@mly_cohernece:* MLy Coherence Model developed with Dragonn @MLy.
  - *@tokenisation_diagram:* The process of conditioning text data for input into a deep learning model. Text data is not intrinsically digestible by artificial neural network models, as artificial neurons can only process numerical inputs. Therefore, in order to apply deep learning models to text data, we must have some method of converting the data into a numerical format @nlp_chapter. Transformers expect a sequence of same-length vectors forming an input matrix, $X$. This diagram shows the process of converting text data into an input matrix. Typically, this conversion is completed in three steps, tokenization, vectorisation, and embedding. However, often, and in the case of the first described transformer model, vectorisation and embedding occur simultaneously, and are often labeled simply embedding @attention_is_all_you_need. This is the method depicted in the diagram. In the example, we see the sentence "The quick brown fox jumped over the lazy dog." as it is prepared for ingestion by an NLP model. *Tokenisation* is the process of splitting one contiguous sequence of characters into a number of unique discrete tokens, $N$. This can be done at multiple levels but is usually done at the scale of words. Sometimes, especially with longer words, words can be split into multiple tokens, as is seen in this example where the word "jumped" is split into "jump" and "ed" @nlp_chapter. There are numerous algorithms to achieve this, which will not be discussed in detail. Every word, or word subset, within the training dataset, should have a unique token ID. Before running inference on new text data, that data must be tokenized, and each word in the new data will be mapped onto an existing token ID that was generated during the initial tokenisation process. Often some information-low words, known as "stop words", and punctuation are removed during the tokenisation process @nlp_chapter. In the example shown, the words "The", and full stops are removed from the input string. During *vectorisation*, each token is assigned a numerical vector, and *embedding* ensures that this vector is transformed into a meaningful vector space to allow for easier interpretation by the model. There are a number of methods to achieve both of these steps, some of which are simultaneous. In the example shown, each token ID is associated with a vector of tunable weights, as was the case in the first transformer paper. These vectors are randomised at the start of training, but as the process continues, they become tuned to values that represent the information contained by the tokens. In this manner, the vectorisation and embedding steps occur at the same time.
  - *@gw_embedding:* Different embedding possibilities to discretise and embed gravitational-wave time-series data. _Upper:_ "Chunking" method of discretisation, where the input time-series is split into $N$ equal-length segments which can be fed into an attention-based model. This method would seem to have the disadvantage that it could split the waveform at any point, leading to chunks with very different waveform content depending on the waveform offset; it also assumes that the innate interferometer output vector is a good embedding for the attention mechanism, which is not necessarily true. _Middle:_ Embedding with dense layers, this setup is similar to the chunking method, but it applies one or more dense layers to each chunk so that the model can learn an embedding that will be better adapted to the attention mechanism in subsequent layers. Since the parameters of the dense layers are repeated for each chunk, this method is equivalent to a convolutional layer with $N$ filters and no overlap, where $N$ is the size of your embedded vector output. _Lower:_ Embedding with convolutional layers. This type of embedding involves creating feature maps of the input vector using a combination of convolutional and/or pooling layers. It is the equivalent of attaching a CNN head at the front of your model. The output of a 1D CNN would be a 2D matrix where one dimension, the depth, is different features, and the other is time. This can then be split into discrete vectors by splitting it along the time dimension to create vectors of features with length equivalent to the number of features.
  - *@bag_of_words:* A "Bag of words". Without ordinality, the meaning represented by this sentence becomes significantly harder, if not impossible, to parse. If we had not already seen this sentence then we would not know if the fox was lazy or quick, or rather if it were the dog that was lazy or quick, and just who is jumping over whom? There are NLP models that are designed to use a bag of words as inputs, but it is easy to see that much information is lost when word order is discarded, thus we can infer that the order and position of the words contain a significant amount of information. The same can be true for time series, a CBC signal that contains a merger, an inspiral, and a ringdown, in that order, can probably be discounted as a glitch, but if we feed it in as a bag of words model, there could be no distinction between this and the expected arrangement.
  - *@weights_matricies:* Generation of query, key, and value vectors for each element in the input sequence of length, $N$. Before attention scores are calculated, each input vector, $accent(x, arrow)_i$ is dotted with the learned query, $W_q$, key, $W_k$, and value, $W_v$, weights projection matrices to produce a query, $accent(q, arrow)_i$, key, $accent(k, arrow)_i$, and value $accent(v, arrow)_i$ vector for the input element, $accent(x, arrow)_i$. This operation is equivalent to the multiplication of the projection matrices and the input matrix, X, to produce the query, $Q$, key $K$, and value $V$ matrices. The key takeaway is that the only tunable parameters are contained in the weights matrices, which act as projection functions to convert the input vector into functional vectors.
  - *@qkv_diagram:* Illustration of example query, key, and value vectors generated for the sentence "The quick brown fox jumped over the lazy dog.". After tokenisation and embedding, each vector in the embedded input sequence generates its own query, key, and value vector. Which together form query, key, and value matrices.
  - *@alignment-diagram:* Illustration of the operation of how the alignment function utilizes the query and key vectors to produce alignment scores for each sequence element. In dot-product attention @attention_2, this is achieved using @alignment-eq. Note that the numbers used here are for illustratory purposes only and not extracted from a real model.
  - *@scaling-diagram:* Illustration of how the alignment scores are used to scale the respective value vectors for each sequence element, and are then summed to produce a new vector that contains global information embedded contextually. Each value vector is multiplied by the respective score, and then these scaled elements are summed together to produce the new vector.
  - *@attention-diagram:*  Illustration of the operation of a single attention head. Here a very small three-element sequence is examined. Each element of the original input sequence is colored differently, in red, green, and blue. All vectors and scalars associated with an input element are colored similarly. The output sequence vectors are colored with a mix of the input colors to show their new information content which consists of distilled global information. More detailed descriptions of the processes shown can be found in @weights_matricies, @alignment-diagram, and @scaling-diagram.
  - *@attention-network-diagram:* _Upper:_ Alternate method of visualizing attention mechanism as a network diagram. Although this is more similar to how networks have been displayed elsewhere in the thesis, it might obfuscate some aspects of the reasoning behind the attention layer operation. As in the @attention-diagram, this illustrates the operation of the attention mechanism on a sequence of length three, with each input vector colored differently, in red, green, and blue. In this representation, the projection matrices, $W_q$, $W_k$, and $W_v$, are represented as dense layers, which are applied to each of the column vectors that comprise the input matrix in turn. It should be noted that although the dense layers are colored differently as they are applied to each input element, this is just to show the different data flows, the weights are maintained by each application of each dense layer. The key, query, and value-dense layers, however, have different weights, and notably, no activation function, as they are just supplying a linear mapping rather than any more complex behavior. _Lower:_ Abstraction of a single attention head layer, that will be used in future diagrams of models which contain attention layers, in order to limit diagram complexity.
  - *@multi-head-diagram:* _Upper:_ Network diagram of multi-attention head. Similar to how multiple convolutional kernels work in tandem in convolutional layers, multiple attention heads work together in multi-attention heads to focus on different information aspects of the input vector. These are then concatenated along the feature axis before finally being multiplied by a further weights matrix, here shown as a dense layer, which serves to mix the output of the different heads and to reshape the output to a desired size. _Lower:_ Abstraction of a multi-head attention layer, that will be used in future diagrams of models which contain attention layers.
  - *@attention-block-diagram:* Typical attention block comprising multiple layers. Residual attention blocks vary in design between architectures but usually maintain the consistent elements shown. The skip connection is here represented by the encircling arrow, which shows that the input of the block is fed to the output before it is returned. There are also several regularisation methods present, batch normalisation, and dropout which help to reduce overfitting and ensure that values within the network remain bounded. Finally, the addition of dense layers and activation functions ensures that non-linear computation can be performed. Sometimes, if a reduction in total model parameter count and inference time is required, convolutional layers can be used in place of dense layers. The question marks indicate user-selectable hyperparameters. 
  - *@transformer-diagram:* The transformer model described by Vaswani _et al._ @attention_is_all_you_need. This encoder-decoder architecture can be used to generate predictions of the next token in a sequence. In the case of @attention_is_all_you_need, this sequence was natural language.
  - *@skywarp_pure_attention:* Skywarp pure attention model with dense embedding. This model architecture was used to test the purest application of attention layers to the detection problem by removing any convolutional layers within the model. The single convolutional layer was employed to increase the input dimensionality of the sequence elements from 16 to 128; this was necessary in order to add positional encoding of the appropriate size. Without positional encoding, models were almost impossible to train. The other hyperparameters were obtained using a rough trial-and-error search of the parameter space. Using a more sophisticated hyperparameters search, though a desired goal, proved difficult due to the increased computational requirements of attention-based models over CNNs. (Attention layer's memory usage scales approximately quadratically with input dimensionality, $N$: $O(N^2)$ @transformer_memory_usage similar to dense layers, as opposed to CNNs, which scale linearly with input dimensionality, $N$, and the number of filters, $F$, in the layers: $O(N times F)$ @cnn_memory_usage, so we encountered so difficulty fitting attention models in memory compared to CNNs.)
  - *@skywarp_conv_attention:* Skywarp convolutional attention with convolutional embedding. This model architecture was employed to test if a convolutional embedding scheme, using the proven CNN architecture of Gabbard _et al._ @gabbard_messenger_cnn embeds the input into feature maps which could then be input into attention layers. We have the greatest success with this model variety, again hyperparameters were manually tuned, so it is expected that with a more thorough hyperparameter investigation, a superior model could be found. 
  - *@skywarp_far_curve:* Model False Alarm Rate (FAR) vs score threshold required to achieve that false alarm rate for the three Skywarp models and the recreated CNN model from Gabbard _et al._ @gabbard_messenger_cnn The four models display notably different FAR curve profiles, though it is important to note that a lower curve on this plot will not necessarily translate to model performance as it says nothing about the True Positive Rate, a classifier that labels everything as noise, for example, would be entirely flat on this plot, but would remain useless. Still, there is a notable difference between the curves, the pure attention model, consistently requires a much higher score threshold than the other three models, which will be seen in its poor efficiency performance.
  - *@skywarp_efficiency_curves:* Efficiency curves of the Skywarp models and the recreated model from Gabbard _et al._ at different False Alarm Rates. For each of the 4 models trained, 61 efficiency tests are performed at evenly spaced optimal SNR values between 0 and 15. For each test, 16384 examples with signals of the relevant SNR are examined by the model. The most evident distinction is between the pure attention model utilizing dense embedding, and the other models, which are either purely convolutional or have a convolutional head. There is considerably less distinction between the other three models, which appear statistically indistinguishable at FARs of $10^(-1)$ Hz and $10^(-2)$ Hz. A slight advantage may arise between the combined attention-convolution model and the other two competitive models at the $10^(-3)$ Hz, which is perhaps the strongest evidence of an advantage, but this small difference is still too small to draw any definite conclusions of improved efficacy. At $10^(-4)$ Hz, the difference is much more apparent, but we are approaching 32-bit precision limits, so it is unclear exactly how seriously we should view these results. The efficiencies at this low FAR is also considerably reduced, reducing the appeal of the use of these models at this FAR. _First:_ Efficiency curves at a FAR of #box($10^(-1)$ + h(1.5pt) + "Hz"). _Second:_ Efficiency curves at a FAR of #box($10^(-2)$ + h(1.5pt) + "Hz"). _Third:_ Efficiency curves at a FAR of #box($10^(-3)$ + h(1.5pt) + "Hz"). _Fourth:_ Efficiency curves at a FAR of #box($10^(-4)$ + h(1.5pt) + "Hz").
  - *@skywarp_roc_curves:* Receiver Operator Curves (ROCs) generated for each of the four Skywarp models for a variety of different SNR pools. The story demonstrated by these plots is very similar to what is shown by the efficiency curves, @skywarp_efficiency_curves, albeit with less granularity. The pure attention model performs considerably worse than the other three models, which are nearly indistinguishable. The pure convolution model has a slightly higher area under the ROC curve, primarily boosted by higher efficiencies at higher false alarm rates and the combined convolutional-attention network achieves higher accuracies at the lower FAR ranges, with the single-layer convolutional-attention network somewhere between the two. _First:_ ROC generated with a pool of signals with optimal SNRs drawn uniformly between 8 to 20. _Second:_ ROC generated with a pool of signals with optimal SNRs of 12. _Third:_ ROC generated with a pool of signals with optimal SNRs of 10. _Fourth:_ ROC generated with a pool of signals with optimal SNRs of 8. _Fifth:_ ROC generated with a pool of signals optimal SNRs of 6.
  - *@overlap_injection_examples:* Two illustrative examples of 1the example used to train CrossWave, the upper demonstrates the single signal case, the lower the multiple signal case. Since the real data used to train CrossWave was unwhitened, it is not easy to parse by eye. Thus, as an illustrative example, these two examples are shown in whitened data generated using cuPhenom and GWFlow. The example duration has also been cropped from #box("16" + h(1.5pt) + "s") to #box("5" + h(1.5pt) + "s"), since the merger times never have a separation greater than #box("2" + h(1.5pt) + "s") this is ample as an example. Both examples show time series from both detectors, simulating LIGO Livingstone and LIGO Hanford. _Upper:_ Single waveform injected into noise drawn from the two LIGO detectors. _Lower:_ A pair of waveforms injected into noise drawn from the two LIGO detectors. The waveforms are always injected with merger times less than #box("2" + h(1.5pt) + "s") distant.
  - *@overlapnet_classification_scores:* Classification error of Overlapnet output when fed validation examples, plotted with signal A optimal network SNR and signal B optimal network SNR. A total of $4 times 10^4$ validation examples were used to produce this plot. All examples consist of two-channel synthetic detector noise generated by colouring Gaussian white noise with the LIGO Hanford and LIGO Livingston aLIGO design specifications. Half the validation examples were injected with one each of $2 times 10^4$  IMRPhenomTPHM waveforms with no repetitions, these are the single injection examples, which only contain Signal A. In these cases the SNR of signal B is always zero, these signals are seen arranged along the bottom of the plot. The other half of the examples consist of two each of the same $2 times 10^4$ IMRPhenomTPHM waveforms with two repeats of the same pairs of signals injected into different noise realizations. A model score near one indicates the model has determined that the example has two hidden signals and a score near zero indicates that the model thinks the example has only one hidden signal. The classification score error shows the difference between the ground truth value and the predicted model output. Therefore an error nearer zero indicates good model performance, and an error nearer one indicates poor model performance. Assuming a classification threshold of 0.5 we can see that the model can successfully classify almost all single examples, and can successfully classify most pairs of signals when the Network SNR of both signals is above an optimal SNR of ten. We note that although classification is achieved in most cases, there is still substantial error in many cases, though mostly below the threshold required for an inaccurate classification, 0.5. It is theorised that this is because the model is trained with many examples of pairs of detectors with one low SNR that are hard to distinguish from single detectors with one signal. This confusion could add considerable uncertainty to the model predictions, and it is recommended that if this project were to be repeated the minimum SNR threshold for both of the signals should be increased. When either of the optimal network SNRs of one of the signals falls below 10, the rate of classification error increases in a curve that is consistently shaped with the detection efficiency curves discussed in previous sections. This is anticipated --- in the case that one of the SNRs becomes low, the signal will appear to look like a single signal as the other signal becomes hard to distinguish. In the case where both signals have a low SNR, both signals are hard to distinguish and it becomes difficult to differentiate between a single hard to identify signal and multiple hard to identify signals. In this latter case, where both signals have a low SNR, the model appears to favour classification as a single signal rather than double. It is hypothesized that this may be because the pairs and single examples were not normalized to have consistent excess power, meaning that the total excess power contained in the set of all two signal examples will be double that of the total excess power in all single signal examples. This might bias the network to associate low excess power with single signal examples. _Left:_ Full validation results. _Right:_ Zoomed result for increased detail below optimal network SNRs of 50.
  - *@overlapnet_efficiency_plots:* Overlapnet pair efficiency plots created from the combined overlapnet validation data pool using rolling averages with a window of 1000 and plotting every 100#super("th") average value. This plot gives an approximate comparison to the efficiency plots generated in the detection cases; although generation was done with a rolling average over a pool of logarithmically distributed SNRs rather than with pools of discrete SNRs at specific test SNR values that have been used previously in the detection cases. Also note that this plots the model output score, rather than the percentage of cases which fall above a calibrated SNR threshold. These efficiency plots show the relationship between the SNR values of one of the signals and the model prediction. One of the five lines gives the rolling average model score when the validation results pool is sorted by minimum SNR value. This is perhaps the most useful of the four lines as it is the bottleneck in classification ability. It reaches a classification score of 1.0 at a minimum optimal network SNR of around 37. It remains above 0.9 for SNRs above 19 and increases slowly until 37. This separates it from the detection case and is presumably because there are extra factors not accounted for on this plot, primarily the SNR of the second signal, but also things like the parameter space difference of the two waveforms and the merger time separation of the two waveforms, which could both add increased difficulty without being visible on the efficiency plot. Two of the lines plot the rolling average model score when plotted with the SNR of one of the two signals, signal A and signal B. Signal B always arrives before signal A. The similarity between these lines shows that it is unlikely there is any bias between whether signal A has the lower SNR or signal B. The maximum scores achieved by these lines are less than the minimum, as there are always low SNR signals in the average used to calculate this. The last of the four pair example lines shows the moving average when the validation pool is sorted by the maximum SNR of the two injected signals. This is the lowest still, as it is more likely that the uncounted-for signals have low SNR. Lastly, we show the single signal SNR scores. Unlike the other signals, a lower score is better in this case, as a model prediction of zero indicates the lack of a second signal. We see that at low SNRs this score is lowest; this is expected as there are considerably more low SNR single signals in the dataset than pairs of signals, and this supports our hypothesis that the network is using excess power as a differentiation method. Above an optimal network SNR of 18 the classification score plateaus at an average of 0.2, as stated previously it is believed this is induced through confusing examples in the training dataset where it is almost impossible for the network to determine between a pair of signals where one signal has a low SNR and a single signal, teaching the network to guess with some uncertainty in all apparent single signal cases. We also see a slight decrease in prediction accuracy as SNR increases, again this probably results from the excess power bias. From this plot we can conclude that as expected the lowest SNR signal in the pair is the largest factor in model efficiency, but that other factors are probably also relevant.
  - *@overlapnet_classification_separation:* Overlapnet classification results plotted against the time elapsed in seconds between the arrival of the merger of signal B and signal A. The coloured circles represent individual validation classification results colour-coded for visual clarity. The red line is the moving average model prediction error at the stated time separation with a window of 500 validation examples. Only pairs are plotted, as single examples have no time separation. We see that for time differences above #box("0.8" + h(1.5pt) + "s") the separation has little effect on the average prediction error. Between #box("0.2" + h(1.5pt) + "s")  and #box("0.8" + h(1.5pt) + "s") there is a slight but notable increase in error, and below a merger time difference of #box("0.2" + h(1.5pt) + "s") there is a more notable uptick in error. It appears that this uptick at lower time separations is mostly caused by signals that have very low separation ($<$ #box("0.1" + h(1.5pt) + "s")) --- this seems to be the only significant predictor of model performance, other than this, and the small decrease in performance below #box("0.8" + h(1.5pt) + "s") the classifier seems to work with equal efficiency across time separations. This is perhaps less of a correlation than might be expected, but it demonstrates that only very close signals are problematic if at detectable SNRs. This is a good sign for the chances of developing a useful classifier. 
  - *@overlapnet_classification_mass_parameters:* Overlapnet classification results compared with the mass parameters of the constituent waveforms. _Left:_ Overlapnet classification scores plotted with source chirp masses for signal A and signal B. There appears to be some correlation between lower chirp masses and poor model performance, however, because there are highly performing examples even in cases where both chirp masses are low we can conclude that this does not explain the entire picture. It is hypothesized, that this correlation is primarily caused by the fact that lower chirp masses are more likely to produce a low SNR signal. If two sources were at the same luminosity distance but one had a higher chirp mass, the higher chirp mass would have a louder SNR (assuming identical detector noise conditions, sky localization, and signal polarisation). This hypothesis is supported by the lower model performance of single signals at higher chirp masses, as we have seen that single signal classification is slightly worse at higher SNRs. _Right:_ Overlapnet classification scores plotted with source mass ratio for signal A and signal B. This plot shows that there is very little, if any correlation between the mass ratio of the two signals, and model performance. This continues to show that signal morphology does not make a decisive difference in classification ability, which is primarily determined by the minimum SNR of a signal in the pairs, and secondarily weakened if the signals have a very small time separation. 
  - *@cross-attention-digaram:* Illustration of the action of a single cross-attention head. In contrast to a self-attention head, a cross-attention head takes two sequences as input: a querier sequence, and a queried sequence. The queryier sequence is converted into query vectors with a learned weights matrix, and the queried sequence is converted into key and value vectors. The rest of the attention mechanism functions identically to self-attention but uses query, key, and value vectors that originate from different sources. For more details on the self-attention mechanism see the description in @sec-attention.
  - *@autoencoder-diagrams:* Illustration of two trivial autoencoder architectures, one using only dense layers, the other using convolutional layers. Both networks have very few neurons and would likely not see use in any real practical application but are presented for illustration only. Autoencoders consist of an encoder that performs dimensional reduction on an input vector to reduce its dimensionality to a smaller latent space and produce a latent vector, this latent vector is then processed by the decoder which attempts to perform the inverse operation and reconstruct the original input image, or a slightly altered version of the input, for example a denoised version of the original input. Often the decoder is simply an inversed version of the encoder, which introduces the concept of transposed convolutional layers which perform the inverted operation of convolutional layers. _Upper:_ Illustrative dense layer autoencoder with a two-layer encoder and a two-layer decoder. The latent space of this autoencoder has two dimensions meaning the dimensionality of the input vector has been reduced from five down to two _Lower:_ Illustrative convolutional autoencoder with a two-layer encoder consisting of convolutional layers and a two-layer decoder consisting of transposed convolutional layers. The latent vector of this autoencoder has four elements, which means there has only been a reduction of one element between the input vector and the latent space.
  - *@crosswave-small-diagram:* Diagram of the network structure of the smaller of the two trialed CrossWave models. Both the CrossWave models have a novel structure with denoising heads, and feature extraction layers adapted from Gabbard _et al._ @gabbard_messenger_cnn, as well as utilization of cross-attention layers. The denoising heads are composed of an autoencoder structure, with one for each input detector. In this case, we have used simulated inputs from the LIGO Hanford and LIGO Livingston detectors so there are two autoencoding heads. Each autoencoder has independently trained weights. It is envisioned that during network training these will adapt individually to the peculiarities of the noise in its given detector, and, due to the shared weights utilized by the feature extractor, learn to output a standardized denoised version of the input from each detector, although it is expected this will not be a perfect correlated to a denoised detector stream since the autoencoders were not independently pre-trained before training of the larger model. After the autoencoding heads, several feature-extracting layers also adapted from Gabbard _et al._ @gabbard_messenger_cnn are used to embed the autoencoder outputs into two sequences that can be ingested by the attention layers. It is imagined, that because these feature-extracting layers share weights between detectors, they will map the output of the denoising layers into a shared latent space that can be interpreted similarly by the attention layers, and is therefore useful for cross-attention between detectors. The core of the small CrossWave model utilizes a repeating block of self-attention layers applied repeatedly to each detector data stream, much like in the Skywarp transformer. These blocks are repeated three times. This repeating self-attention layer should help the model understand the global context of the data within each detector. After completion, these data streams are combined in a cross-attention block, and then processed by two dense layers to give the final regression output scores. This model was trialed and was somewhat performant, but the application of the cross-attention in this method was causing a lot of information to be lost, so the model was abandoned in favour of the larger variant shown in @crosswave-large-diagram.
  - *@crosswave-large-diagram:* Diagram of the network structure of the larger of the two trialed Crosswave models. Both the CrossWave models have a novel structure with denoising heads, and feature extraction layers adapted from Gabbard _et al._ @gabbard_messenger_cnn, as well as utilization of cross-attention layers. The denoising heads are composed of an autoencoder structure, with one for each input detector. In this case, we have used simulated inputs from the LIGO Hanford and LIGO Livingston detectors so there are two autoencoding heads. Each autoencoder has independently trained weights. It is envisioned that during network training these will adapt individually to the peculiarities of the noise in its given detector, and, due to the shared weights utilized by the feature extractor, learn to output a standardized denoised version of the input from each detector, although it is expected this will not be a perfect correlated to a denoised detector stream since the autoencoders were not independently pre-trained before training of the larger model. After the autoencoding heads, several feature-extracting layers also adapted from Gabbard _et al._ @gabbard_messenger_cnn are used to embed the autoencoder outputs into two sequences that can be ingested by the attention layers. It is imagined, that because these feature-extracting layers share weights between detectors, they will map the output of the denoising layers into a shared latent space that can be interpreted similarly by the attention layers, and is therefore useful for cross-attention between detectors. The core of the larger CrossWave block contains both self-attention blocks and cross-attention blocks in each iteration, this means that the model can compare data streams from both detectors multiple times, each time adding extra relevant information from the other detector into that detector's branch. Also, since the cross-attention is being performed in both directions, no information is lost as it was in the small model. Again, these blocks are repeated three times. After the repeating blocks, rather than using a cross-attention block to combine the branches, the outputs from each branch were concatenated before being fed into the dense tail, which then produced the final regression outputs.
  - *@crosswave_regression_snr:* CrossWave merger time prediction error of Signal A, _upper left_, and Signal B, _upper right_. Compared to the classification results, the merger time errors look more consistent. This is primarily because the model output is not restricted between one and zero like as it is in classification, so a few outliers with very high errors saturate the color map. Given this, we have also plotted the same results with all examples that have errors greater than #box("0.25" + h(1.5pt) + "s") removed, for a more granular view of the bulk of the regression prediction errors. These are the lower two plots. In these focused plots, we can see that a significant number of results have a regression error of less than #box("0.1" + h(1.5pt) + "s"), which could be helpful to aid a secondary parameter estimation method. On these lower plots, there is also a notable difference between the average error on signal A merger time predictions, and the average error on signal B merger time predictions, with a higher average error on signal B. It is unclear exactly why this is the case, but we speculate that this is because signal B arrives first in the detector, meaning that the inspiral of signal A can interfere significantly with signal B, whereas the opposite only the case when the merger separation is very small. It is also possible that sometimes, signal A can be misclassified as signal B. We would expect this latter confusion to have some correlation to SNR, but this does not seem to be the case. It could also be due to the aforementioned normalisation error reducing model training efficacy for signal B merger time predictions. Interestingly, the relationship between signal SNR and regression error appears low. This suggests that the substantive cause of regression error lies elsewhere, we plot additional comparisons to further investigate.
  - * @crosswave_regression_efficiencies:* CrossWave rolling average merger time prediction error plotted when ranked by different SNR combinations. Since the model now has two outputs, one for each merger time in the input example, a plot was generated for each merger time prediction. A plot showing signal A merger time prediction on the left, and a plot showing signal B merger time prediction on the right. At low SNR, the error is dominated by the SNR in the given signal, which is anticipated --- a low SNR in a given signal would, evidently, make it difficult for the model to detect, and hence, estimate the merger time, of that signal. We can also see the notable difference in average prediction error between the upper signal A plot and the lower signal B plot. Interestingly, we see that the error on the signal B merger time increases when the SNR of signal A is higher. This seems to be the case regardless of the SNR of signal B. Since signal B always arrives first in the detector, this could be because a loud signal A inspiral obfuscates the presence of signal B, rendering the signal B merger time hard to identify.
  - *@crosswave_merger_times:* CrossWave merger arrival time prediction errors compared with the time separation between signal A and signal B merger arrival times in the LIGO Hanford detector. _Left:_ Error on signal A merger time prediction compared with the time separation between the two mergers. _Right:_ Error on signal B merger time prediction compared with the time separation between the two mergers. The colour of the plotted examples depicts the absolute error between the model prediction and the ground truth value, and the red line shows the rolling average absoloute prediction error. For both merger times, we can see a spike in erroneous merger time predictions when the time separation is near zero. This is similar behavior to what is seen in the classification examples. It is also expected here, since if the mergers are hard to distinguish from each other it will be difficult to determine the specific merger times. An asymmetry arises in which way the model will incorrectly predict the merger, in signal A, defined as the second to arrive in the detector, the model often predicts the signal will arrive later than it does, and for signal B, the model often thinks it will arrive earlier than it does. Since B always arrives first, these are logical assumptions for the model to make in both cases. In both cases, we also see lines of erroneous predictions where the model error equals the time separation. These are believed to be cases where the model believes signal A to be signal B and vice versa. This line is more pronounced for signal B errors, suggesting that signal B's are more commonly mistaken for signal A's than the other way around.
  - * @crosswave_arrival_time_prediction_error:* CrossWave signal merger time parameter estimation results. Each pair of plots shows the merger time estimate of signal A (_left_) and signal B (_right_). For each validation example, the ground truth value is represented on the x-axis, and the model prediction is on the y-axis. Each represents the signal merger time in seconds. The colour of each circle depicts the absolute difference between the ground truth value and the model prediction, which will be zero if the point falls on the line of $x = y$, which is also shown on the plot as a dashed grey line. Due to an error in label normalisation, some ground truth values for signal B were less than zero. Unfortunately, due to the choice of loss function used for the regression (ReLU), the model could not output predictions below zero, this meant that it was unable to predict these values correctly. This error may have interfered slightly with the rest of the training process, however other than its inability to classify these examples, there does not seem to be a significant reduction in the performance of classification of signal B merger times. Validation examples with round truth values below zero, and their associated predictions have been omitted from signal B plots for visual clarity. If training were to be repeated this error could be easily rectified, either by correcting the normalization or by altering the choice of activation function. _Upper Left:_ Predicted against actual signal A merger time in the simulated LIGO Hanford output. _Upper Right:_ Predicted against actual signal B merger time in the simulated LIGO Hanford output. _Lower Left:_ Predicted against actual signal A merger time in the simulated LIGO Livingston output. _Lower Right:_ Predicted against actual signal B merger time in the simulated LIGO Livingston output. 
  - *@crosswave_mass_prediction_error:* CrossWave companion mass parameter estimation results. Each pair of plots shows the companion mass estimates of signal A (_left_) and signal B (_right_). For each validation example, the ground truth value is represented on the x-axis, and the model prediction is on the y-axis. Each represents the companion mass in solar masses. The colour of each circle depicts the difference between the ground truth value and the model prediction, which will be zero if the point falls on the line of $x = y$, which is also shown on the plot as a dashed grey line. After the merger time predictions, the mass plots show the greatest promise, able to predict component masses with a moderate degree of accuracy. Without a comparison to another parameter estimation method, it is unclear exactly how much use these results can be. _Upper Left:_ Predicted against actual signal A companion 1 mass. _Upper Right:_ Predicted against actual signal B companion 1 mass. _Lower Left:_ Predicted against actual signal A companion 2 mass. _Lower Right:_ Predicted against actual signal B companion 2 mass.
  - *@crosswave_spin_error:* CrossWave regression results for the dimensionless spin components of the two companions in each binary merger, A and B. The left plots show the parameter extracted from merger A, whereas the right results show the same parameter extracted by CrossWave from merger B. The plots show the ground truth value of the dimensionless spin component plotted against the predicted value of the dimensionless spin component. The colour of each validation example indicates the difference between the ground truth and the predicted value, in this case, equivalent to the distance the point is from the line of $x = y$.
  - *@crosswave_luminosity_distance_error:* CrossWave model predicted luminosity distance vs ground truth luminosity distance of simulated BBH waveforms. _Left:_ Predicted signal A luminosity distance. _Right:_ Predicted signal B luminosity distance. The colour of each example point indicates the difference between the predicted and the ground truth value for that example. These plots indicate that there is almost no correlation between the predicted luminosity distance and the ground truth value. The model outputs a very similar value independent of luminosity distance, it is unclear whether this inability arises from a problem with model training and/or data processing, or whether luminosity distance is too difficult for the model to determine because of degeneracy with other parameters such as inclination.

#pagebreak()

// Tables
= List of Tables

- *@detector_data_table:* The frame, channel, and state flags used when obtaining data from the respective detectors during the 3#super("rd") observing run (O3). This data was used as obfuscating noise when generating artificial examples to train and validate artificial neural network models throughout this thesis. It should be noted that although the clean channels were produced offline in previous observing runs, the current observing run, O4, produces cleaned channels in its online run, so using the cleaned channels during model development ensures that the training, testing, and validation data is closer to what would be the normal operating mode for future detection methods.
- *@feature-enginering-types:* A non-exhaustive table of possible data conditioning modes. Feature engineering is often used in order to simplify a problem before it is presented to a machine learning model. There are many ways we could do this with gravitational-wave data. Presented are some of the most common. Each is described in more detail in @feature-eng-sec.
- *@perceptron-training-parameters:* The common training and dataset hyperparameters shared by the CBC and Burst perceptron experiments. Note that the scale factor here refers to the factor used during the upscaling of the CBC waveforms and real interferometer noise from their extremely small natural dimensions to make them artificial neuron-friendly. This is done both to ensure that the input values work well with the network activation functions and learning rates, which are tuned around values near one, and to reduce precision errors in areas of the code that use 32-bit precision, employed to reduce memory overhead, computational cost and duration. Data acquisition batch duration is a parameter of the GWFlow data acquisition module @gwflow_ref. For speed, the GWFlow data acquisition system downloads data in larger segments than is required for each training batch, then randomly samples examples from this larger segment to assemble each training batch. The data acquisition batch duration determines how long this larger batch is. Smaller values will result in a more evenly mixed training data set and a lower overall GPU memory overhead but will be more time-consuming during the training process. 
- *@literature-results:* A comparison of results from the literature, red values indicate the significant feature of the study. Note: Some accuracy values are extracted from plots by eye, so substantive error will have been introduced. Some results were not included as they did not state comparable performance metrics. 
- *@datset-hyperparameters*: Possible Dataset Hyperparameters. These are parameters that alter the structure and composition of the dataset used to train or model. None of these parameters were selected for inclusion in our hyperparameter optimization test, in order to decrease convergence time. Parameters with a superscript symbol become active or inactive depending on the value of another parameter in which that symbol is contained within brackets. Range entries are left black for Hyperparameters not included in optimisation, as no ranges were selected for these values.
- *@training-hyperparaneters:* Possible Training hyperparameters. These are parameters that alter the training procedure of the model. None of these parameters were selected for inclusion in our hyperparameter optimization test, in order to decrease convergence time. Parameters with a superscript symbol become active or inactive depending on the value of another parameter in which that symbol is contained within brackets. There are different optimiser parameters that could also be optimized depending on your choice of optimiser, for example, values for momentum and decay. It is not typical to optimise your choice of loss function for most tasks, but some are possible with a range of loss functions, such as regression, which could benefit from optimisation of this parameter. Range entries are left black for Hyperparameters not included in optimisation, as no ranges were selected for these values.
- *@architecture-hyperparameters:* Possible Architecture hyperparameters. These are parameters that alter the arcitectural structure of the model, or the internal structure of a given layer. All these parameters were selected for optimisation. Parameters with a superscript symbol become active or inactive depending on the value of another parameter in which that symbol is contained within brackets. For each of $N$ layers, where $N$ is the value of the number of hidden layers genome, a layer type gene detemines the type of that layer, and other hyperparameters determine the internal structure of that layer.
- *@top_model_perfomances:* 
- *@skywarp-results-table:* Accuracy results at different optimal SNRs from the four models tested at different FAR thresholds. Scores in red are the highest results for that SNR threshold at that FAR, in the one case where there is a tie, both scores are highlighted. With a very marginal lead, the single-layer attention-convolution hybrid appears the be the best model at a FAR of #box($10^(-1)$ + h(1.5pt) + "Hz"), only losing out to the CNN and deeper hybrid model by 0.1 percentage points at the highest SNR of 10. This is not a particularly useful FAR range, however, and as we decrease the FAR, the deeper attention layer seems to be victorious, but again the lead is quite small. This appears to show that the convolutional-attention model may have a slight advantage at lower FARs. At the lowest FAR presented, #box($10^(-4)$ + h(1.5pt) + "Hz"), the required score threshold for both convolutional-attention hybrid with the single attention layer, and the pure attention model, have reached one, and therefore lack any classification ability. For the remaining models, the required model score threshold is greater than 0.999, and although at this FAR the deep attention hybrid appears to be the clear winner, it is unclear whether victory at such a high score threshold is meaningful, or simply due to statistical variance in model training and the validation dataset. Although it should be noted that the lead is similar in all SNR bins, which were created independently, so if it is statistical variance, it is suggested that it probably originates in the training procedure.
- *@skywarp-results-table:* Accuracy results at different optimal SNRs from the four models tested at different FAR thresholds. Scores in red are the highest results for that SNR threshold at that FAR, in the one case where there is a tie, both scores are highlighted. With a very marginal lead, the single-layer attention-convolution hybrid appears the be the best model at a FAR of #box($10^(-1)$ + h(1.5pt) + "Hz"), only losing out to the CNN and deeper hybrid model by 0.1 percentage points at the highest SNR of 10. This is not a particularly useful FAR range, however, and as we decrease the FAR, the deeper attention layer seems to be victorious, but again the lead is quite small. This appears to show that the convolutional-attention model may have a slight advantage at lower FARs. At the lowest FAR presented, #box($10^(-4)$ + h(1.5pt) + "Hz"), the required score threshold for both convolutional-attention hybrid with the single attention layer, and the pure attention model, have reached one, and therefore lack any classification ability. For the remaining models, the required model score threshold is greater than 0.999, and although at this FAR the deep attention hybrid appears to be the clear winner, it is unclear whether victory at such a high score threshold is meaningful, or simply due to statistical variance in model training and the validation dataset. Although it should be noted that the lead is similar in all SNR bins, which were created independently, so if it is statistical variance, it is suggested that it probably originates in the training procedure.
- *@overlaping-event-rate:* Estimated overlap rates of BBH signals in current and future detectors, sourced from Relton @phil_thesis. Presented error values are 90% credible intervals. Note that these results, including past observing runs are estimates rather than real values, and are meant only as an illustration of the probable difference in overlap rates between current and future detector configurations. The number of overlapping signals, $N_"overlap"$, anticipated within one year is determined by the number of detections, $N_"events"$, and the visible duration of those detections, which are, in turn, affected by the detection range and lower frequency cut off the detector configuration in question. We can see that although with the current and previous detector configurations an overlapping event is extremely unlikely, it will increase with LIGO-Voyager to the point where we would expect $6.3_(-3.4)^(+7.7)$ overlapping signals per year of observing time, and further increase with the Einstein Telescope to the point where we would not expect any event to be detected without components of other signals also present in the detector. Similar overlaps are expected for LISA and Cosmic Explorer.
- *@crosswave-training-parameters:* The training and dataset hyperparameters used in CrossWavea and Overlapnet experiments.
- *@crosswave-regression-results:* Results of the CrossWave parameter estimation model. For each of the model's outputted parameters, a Mean Absolute Error (MAE) along with an $R^2$ score is presented. The MAE indicates the average magnitude of the errors between the model's predictions on the validation dataset and the corresponding ground truth values. It is a measure of average prediction accuracy, though it doesn't distinguish between overestimation and underestimation. The $R^2$ score quantifies how well the model's predictions explain the variance of the ground truth values in the validation dataset. An $R^2$ score of one signifies perfect prediction accuracy in the validation examples used. In contrast, a score of zero suggests the model's predictive capability is no better than simply using the mean value of the validation examples. Negative $R^2$ values indicate that the model performs worse than a model that would always predict the mean, possibly signaling errors in the training process or model selection. 
 
#pagebreak()

// Collaborative Work
= Collaborative Work

A small amount of the work presented in this thesis was perfomed in collaboration with others. Here is a list of the sections that contain some collaborative work:

- Several of the figures used in @gravitational-waves-sec were produced by Meryl Kinnear using her impressive knowledge of Mathematica. Specifically, these were: @flat, @gravitaional-potentials, and @waves which is also the image on the half-cover page. This was done as a favour to me and is greatly appreciated.
- Although the work done to train the MLy models using the genetic algorithm-based hyperparameter optimisation method presented in this these was not strictly collaborative, it is described in @deployment-in-mly as an example use case of the method. Work to optimise and train these models was performed solely by Vasileios Skliris, with whom I have collaborated in the past on development work for the MLy pipeline, but not for any of the work presented in this thesis other than what is mentioned in @deployment-in-mly.
- The work presented in @crosswave-sec was collaborative. The problem was presented, and the training and testing datasets were generated by Philip J. Relton. I performed all the work to create and train the models, although some guidance on the nature of the problem and the importance of different aspects was provided by Phil. Our collaboration extended only to the first of the models presented, Overlapnet, after which Phil left the project. The parameter estimation model, CrossWave, was a continuation of the concept, and the same training and validation datasets generated by Phil were used, however, there was no further input from Phil in the training of CrossWave. All data analysis was performed independently, although again, the importance of certain aspects of the problem had previously been highlighted by Phil.

#pagebreak()

// Acknowledgments
= Acknowledgments

A great deal of people, fortuitous events, and luck were required to create the conditions necessary for the production of this thesis. It would be a hopeless task to try and name them all --- although perhaps I could train a neural network to do it. Nonetheless, I will attempt to highlight and express my overwhelming gratitude toward the most crucial contributors. Note that this is not in a particular order, so please don't try and rank yourselves.

Firstly, I would like to thank my supervisor Patrick Sutton. He has helped improve my skills as a scientist in innumerable ways and managed to direct me whilst still providing me with the freedom to explore my own ideas. I imagine achieving the balance between the correct levels of trust and guidance must be one of the most difficult parts of being a Ph.D. supervisor, but he has managed to excel at the task. I owe a great deal of the wide range of skills I have developed over the course of the PhD to him.

Of course, I need to cover the basics, although thankfully in this case it is not down to a sense of obligation. I have the most supportive family anyone could ever wish for. I am fortunate enough that there has never been a single moment of doubt, a single question as to whether this is a thing I could do, and whether it is a thing I should do. They have always been there, in the background, to support me and let me know that even if things didn't work out, it would be okay in the end. I give special thanks to my father, Mark, who has always shown an interest in my work and even attempted to read this thesis, and my mother Caroline, who sends me an advent calendar every year.

Speaking of support, I would be remiss not to mention my source of funding, AIMLAC, (it's a terrible acronym, I won't do it the dignity of expanding it), and the wonderful people who run it. I have made many hopefully enduring connections through the myriad of conferences and events they put on for us, both academically and socially. Through AIMLAC, I have met many people whom I now consider friends, including Tom, Hattie, Ben, Cory, Maciek, Sophie, Robbie, and Tonichia.

Perhaps the largest contribution to the ideas behind this project, and the institution of gravitational waves machine learning research at Cardiff, comes from 
Vasileios Skliris. He was Patrick's student prior to me and paved much of the way that this work follows. Despite having to deal with me and my constant stream of new ideas, he continues to push for real applications of machine learning methods with his development of the MLy pipeline.

Next, we come to those who have supported me beyond an academic sense, but whose roles have been of equal importance. Without them, the thought of four years of Ph.D. work is almost incomprehensible (although maybe I could have got it done in two if I didn't have anyone in the office that I could distract). There are a great many people who fit into this category and I will certainly forget some of them (plus the thesis is long enough as it is). Firstly, Phil, Zoltan, and Sander, you kept me sane with many opportunities for terrible coffees, shots of gin, and opportunities to rant about A.I. destroying the world. I already miss you both. I'd also be remiss not to mention all those who came before me, including Ali, Johnathan, and Virginia, who are both hopefully enjoying the sunshine in California; and all those who will remain here after I'm gone (I swear I'm perfectly healthy), including Abhinav, Debatri, Jordan, Wasim, and of course Sama, who promised to read my thesis when its done (I'm sorry I made it so long. Good luck). I hope that she will continue to work on our join unstarted project, Crocodile. I will also mention friends I have somehow managed to retain from outside gravitational waves, all of whom are very dear to me, Harvey, Luke, Patrick, and Elliot. With special thanks to Elliot, who has been my agony aunt through many situations. Oh, and probably some astronomers too.

Lastly, (I put these three in a separate paragraph because I think it'll be funny if they think I've missed them) thank you to Meryl, Terri, and Eva. Thank you for encouraging me to write an acknowledgments section, for supplying me with a good proportion of my current wardrobe, and for your unsuccessful attempts to help me get over my fear of sauce. You've made this last year a lot less stressful than it could have been.